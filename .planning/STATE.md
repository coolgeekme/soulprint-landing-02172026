# Project State

## Project Reference

See: .planning/PROJECT.md (updated 2026-02-08)

**Core value:** The AI must feel like YOUR AI -- genuinely human, deeply personalized, systematically evaluated.

**Current focus:** Phase 4 - Quality Scoring (IN PROGRESS)

## Current Position

Phase: 4 of 5 (Quality Scoring)
Plan: 1 of 3 in phase
Status: In progress
Last activity: 2026-02-09 - Completed 04-01-PLAN.md (Quality Scoring Infrastructure)

Progress: [█████████░] 91% (Phases 1-3 complete, Phase 4 Plan 1 complete)

## Performance Metrics

**Velocity:**
- Total plans completed: 52 (from v1.0-v2.0)
- Average duration: ~19 min
- Total execution time: ~21.10 hours across 6 milestones

**By Milestone:**

| Milestone | Phases | Plans | Status |
|-----------|--------|-------|--------|
| v1.0 MVP | 1 | 1 | Shipped |
| v1.1 Stabilization | 7 | 22 | Shipped |
| v1.2 Import UX | 3 | 9 | Shipped |
| v1.3 RLM Sync | 5 | 5 | Shipped |
| v1.4 Personalization | 2 | 7 | Shipped |
| v1.5 Full Chat | 6 | 8 | Shipped |
| v2.0 AI Quality | 5 | 5 | In Progress |

*Metrics updated: 2026-02-09*

## Accumulated Context

### Decisions

Decisions are logged in PROJECT.md Key Decisions table.

**Carried forward from v1.5:**
- Separate soulprint-rlm repo -- Production RLM deploys from Pu11en/soulprint-rlm
- Sonnet 4.5 on Bedrock for chat quality (switched from Nova Lite)
- OpenClaw-style prompt: minimal preamble, sections define personality
- Two-pass generation: quick pass (~30s, Haiku 4.5) + full pass (RLM background)
- 7-section structured context (SOUL, IDENTITY, USER, AGENTS, TOOLS, MEMORY, daily memory)

**v2.0 Architecture decisions:**
- Evaluation-first approach: build measurement before changing prompts
- Phase 1 -> Phase 2 dependency: need metrics before prompt changes
- Phases 3 and 4 can run in parallel after Phase 2

**From 01-01 (Evaluation Core Library):**
- ChatEvalItem uses index signature for Opik DatasetItemData compatibility
- zod v4 z.record() requires two args (key type + value type)
- Minimum 10 valid pairs enforced for dataset statistical significance
- BaseMetric extension pattern: validationSchema + score with safeParse guard

**From 01-02 (Experiment Runner & CLI Scripts):**
- V1 prompt builder copied inline in baseline.ts (not extracted from chat route) to freeze baseline snapshot
- Array.from(Map.entries()) pattern for ES2017 target compatibility
- PromptVariant interface: { name, buildSystemPrompt } for pluggable prompt strategies
- CLI scripts use DOTENV_CONFIG_PATH=.env.local with import dotenv/config

**From 02-01 (PromptBuilder Class):**
- PromptBuilder uses PromptParams object pattern (not positional args) for extensibility
- v2 personality sections use flowing prose, only functional sections use ## headers
- v2 section ordering: USER before AGENTS/IDENTITY, CONTEXT before REMEMBER
- getPromptVersion() exported standalone for testing/logging without instantiating builder
- PRMT-04 pattern: behavioral_rules from agents_md reinforced in ## REMEMBER after ## CONTEXT

**From 02-02 (Cross-Language Prompt Sync):**
- Python PromptBuilder mirrors TypeScript character-for-character for v1, v2, and imposter mode
- Injectable currentDate/currentTime params added to TypeScript PromptParams for deterministic testing
- Python _parse_section_safe handles both dict and JSON string inputs
- _sections_to_profile helper bridges legacy sections dict to PromptBuilder profile format
- JSON blob serialization pattern for subprocess param passing avoids shell escaping issues

**From 02-03 (Wire v2 to Evaluation):**
- v2 builder uses simplified PromptParams (no dailyMemory/memoryContext) for style comparison
- recordBaseline() accepts optional variant parameter (non-breaking change, defaults to v1)
- v1 baseline remains frozen (not refactored to use PromptBuilder) for historical accuracy
- ChatEvalItem structured sections JSON.stringify into PromptBuilder *_md fields
- VARIANTS map in CLI scripts for variant registry and name-based selection

**From 03-01 (Emotional Intelligence Foundation):**
- Haiku 4.5 on Bedrock for emotion detection (fast, cheap, 150 max tokens, temp 0.2)
- Fail-safe neutral default on any detection error (never crash chat)
- Confidence threshold 0.6 for adaptive tone application (avoid low-confidence misclassification)
- Adaptive tone goes LAST in prompt composition (base → uncertainty → relationship → adaptive)
- Relationship arc thresholds: early (<10), developing (10-50), established (50+)
- Dynamic temperature ranges: factual (0.2), confused (0.25), creative (0.8), default (0.7)
- EmotionallyIntelligentPromptParams extends PromptParams for backward compatibility

**From 03-02 (Chat Route EI Integration):**
- EI operations run BEFORE RLM call (data available for both paths)
- RLM path does NOT use buildEmotionallyIntelligentPrompt (RLM builds prompts server-side)
- Only Bedrock fallback uses emotionally intelligent prompt composition
- All EI operations wrapped in try/catch with neutral defaults (never crash chat)
- Relationship arc query uses .select('id', {count: 'exact', head: true}) for efficiency
- Opik spans include EI metadata for evaluation correlation

**From 03-03 (Python EI Prompt Sync):**
- Python EI section builders are module-level functions (not class methods) to mirror TypeScript exports
- Python PromptBuilder.build_emotionally_intelligent_prompt mirrors TypeScript character-for-character
- No emotion detection in Python (RLM receives emotional_state as parameter from TypeScript caller)
- Cross-language sync test verifies character-identical output for all EI sections
- Confidence threshold 0.6 enforced in both TypeScript and Python for adaptive tone

**From 04-01 (Quality Scoring Infrastructure):**
- Three separate judge classes (Completeness, Coherence, Specificity) vs unified judge for specialized evaluation
- Score normalization from 0.0-1.0 to 0-100 integer range for JSONB storage efficiency
- Parallel scoring: 5 sections × 3 judges = 15 LLM calls in ~2-3 seconds (vs ~25s sequential)
- GIN index on quality_breakdown JSONB for efficient threshold queries (find_low_quality_profiles)
- CompletenessJudge validates section-specific expected fields (7 for soul, 5 for identity, etc.)

### Pending Todos

- Run `scripts/rls-audit.sql` in Supabase SQL Editor (from v1.1 Phase 4)

### Blockers/Concerns

**For v2.0 planning:**
- Phase 3 needs uncanny valley threshold research (how much mirroring is too much)
- Quality scoring metrics need validation that they correlate r>0.7 with user satisfaction

**Carried forward from v1.5:**
- Web search (smartSearch) exists but citations not validated against hallucination

## Session Continuity

Last session: 2026-02-09
Stopped at: Completed 04-01-PLAN.md (Quality Scoring Infrastructure)
Resume file: None

---
*Last updated: 2026-02-09 -- Phase 4 Plan 1 COMPLETE (Quality Scoring: judges, orchestrator, database migration)*
