---
phase: 09-streaming
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - app/api/chat/route.ts
autonomous: true

must_haves:
  truths:
    - "AI response text streams token-by-token to the frontend (not as a single chunk)"
    - "Long responses (>30s) do not hit Vercel function timeout"
    - "If user disconnects mid-stream, server stops iterating the Bedrock stream"
    - "Existing chat features (web search, memory context, soulprint, AI naming, learning) still work"
  artifacts:
    - path: "app/api/chat/route.ts"
      provides: "Streaming chat API using ConverseStreamCommand"
      contains: "ConverseStreamCommand"
      exports: ["POST", "maxDuration"]
  key_links:
    - from: "app/api/chat/route.ts"
      to: "@aws-sdk/client-bedrock-runtime"
      via: "ConverseStreamCommand"
      pattern: "ConverseStreamCommand"
    - from: "app/api/chat/route.ts"
      to: "ReadableStream"
      via: "Immediate return with async start callback"
      pattern: "new ReadableStream"
---

<objective>
Convert the chat API route from buffered responses to true token-by-token streaming using AWS Bedrock ConverseStreamCommand.

Purpose: Users currently wait for the entire AI response to generate before seeing anything. This plan makes responses appear incrementally, dramatically improving perceived latency. This is the critical backend change that enables STRM-01 and STRM-03.

Output: A rewritten `app/api/chat/route.ts` that streams Bedrock responses via SSE, returns the Response immediately (not after awaiting), checks abort signals, and includes Vercel runtime configuration.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/09-streaming/09-RESEARCH.md
@app/api/chat/route.ts
@lib/bedrock.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Convert chat route to true streaming with ConverseStreamCommand</name>
  <files>app/api/chat/route.ts</files>
  <action>
Rewrite `app/api/chat/route.ts` to use true streaming. The key changes:

**Architecture decision:** Both the RLM path and the Bedrock fallback path need to stream. Since RLM returns complete responses (not streaming), the approach is:
- Still call `tryRLMService()` first for the response (it handles memory retrieval + response generation via its own Bedrock call)
- If RLM succeeds: Stream the complete RLM response in small chunks (simulated streaming) so the frontend always gets the same SSE format. Break the response into ~20-char chunks with small delays to feel natural. This preserves existing RLM functionality while the UI gets a consistent streaming experience.
- If RLM fails (fallback): Use `ConverseStreamCommand` for true token-by-token streaming from Bedrock directly.

**Specific changes to make:**

1. Add imports: `ConverseStreamCommand` from `@aws-sdk/client-bedrock-runtime` (already imported but unused -- keep it)

2. Add Vercel runtime config at the TOP of the file (after imports, before any function definitions):
```typescript
export const maxDuration = 60;
```

3. **RLM success path (lines ~336-370):** Replace the current single-chunk SSE response with a ReadableStream that sends the RLM response in small pieces:
```typescript
if (rlmResponse) {
  // Learn asynchronously (keep existing learning logic)
  if (rlmResponse.response && rlmResponse.response.length > 0) {
    learnFromChat(user.id, message, rlmResponse.response).catch(/* same as now */);
  }

  // Stream RLM response in chunks for consistent UX
  const fullText = rlmResponse.response;
  const stream = new ReadableStream({
    async start(controller) {
      const encoder = new TextEncoder();
      // Send in ~20 char chunks
      for (let i = 0; i < fullText.length; i += 20) {
        if (request.signal.aborted) {
          controller.close();
          return;
        }
        const slice = fullText.slice(i, i + 20);
        const chunk = `data: ${JSON.stringify({ content: slice })}\n\n`;
        controller.enqueue(encoder.encode(chunk));
      }
      controller.enqueue(encoder.encode('data: [DONE]\n\n'));
      controller.close();
    },
  });

  return new Response(stream, {
    headers: {
      'Content-Type': 'text/event-stream',
      'Cache-Control': 'no-cache',
      'Connection': 'keep-alive',
    },
  });
}
```

4. **Bedrock fallback path (lines ~372-455):** Replace the `ConverseCommand` + single-chunk pattern with `ConverseStreamCommand` and immediate Response return:
```typescript
// Bedrock FALLBACK with true streaming
const command = new ConverseStreamCommand({
  modelId: process.env.BEDROCK_MODEL_ID || 'us.anthropic.claude-3-5-haiku-20241022-v1:0',
  system: [{ text: systemPrompt }],
  messages: converseMessages,
  inferenceConfig: {
    maxTokens: 4096,
  },
});

let fullResponse = '';
const stream = new ReadableStream({
  async start(controller) {
    const encoder = new TextEncoder();
    try {
      const response = await bedrockClient.send(command);

      if (!response.stream) {
        throw new Error('No stream in Bedrock response');
      }

      for await (const event of response.stream) {
        if (request.signal.aborted) {
          controller.close();
          return;
        }

        if (event.contentBlockDelta?.delta && 'text' in event.contentBlockDelta.delta) {
          const text = event.contentBlockDelta.delta.text || '';
          if (text) {
            fullResponse += text;
            const chunk = `data: ${JSON.stringify({ content: text })}\n\n`;
            controller.enqueue(encoder.encode(chunk));
          }
        }
      }

      controller.enqueue(encoder.encode('data: [DONE]\n\n'));
      controller.close();

      // Learn asynchronously AFTER stream completes
      if (fullResponse.length > 0) {
        learnFromChat(user.id, message, fullResponse).catch(err => {
          reqLog.warn({ error: err instanceof Error ? err.message : String(err) }, 'Learning failed (non-blocking)');
        });
      }

      const duration = Date.now() - startTime;
      reqLog.info({ duration, status: 200, responseLength: fullResponse.length, fallback: 'bedrock-stream' }, 'Chat stream completed');

    } catch (error) {
      if (request.signal.aborted) {
        controller.close();
        return;
      }
      reqLog.error({ error: error instanceof Error ? error.message : String(error) }, 'Bedrock stream error');
      const errorChunk = `data: ${JSON.stringify({ error: 'Stream failed' })}\n\n`;
      controller.enqueue(encoder.encode(errorChunk));
      controller.close();
    }
  },
  cancel() {
    reqLog.debug('Stream cancelled by client');
  },
});

return new Response(stream, {
  headers: {
    'Content-Type': 'text/event-stream',
    'Cache-Control': 'no-cache',
    'Connection': 'keep-alive',
  },
});
```

5. Remove the old non-streaming Bedrock code (the `ConverseCommand` usage, the `textBlocks` extraction, the old single-chunk SSE return).

6. Keep ALL existing functionality intact:
   - Auth check
   - Rate limiting
   - Request validation
   - Profile loading
   - Memory context retrieval
   - Smart search
   - AI name generation
   - RLM circuit breaker
   - `buildSystemPrompt` function (unchanged)
   - `tryRLMService` function (unchanged)
   - `generateAIName` function (unchanged)
   - `validateProfile` function (unchanged)

7. Move the logging for RLM success path to AFTER the stream completes (inside the start callback, after controller.close()), not before the Response return.

**What NOT to do:**
- Do NOT change the RLM service or its `/query` endpoint
- Do NOT change the system prompt building logic
- Do NOT remove the Bedrock fallback (RLM-first is the current architecture)
- Do NOT use Edge runtime (Node.js is fine with maxDuration=60)
- Do NOT await stream processing before returning the Response (this causes buffering -- the CRITICAL anti-pattern)
  </action>
  <verify>
Run `npm run build` to confirm no TypeScript errors. Verify the file contains:
- `export const maxDuration = 60` near the top
- `ConverseStreamCommand` in the Bedrock fallback path
- `new ReadableStream` with `async start(controller)` pattern
- `request.signal.aborted` check inside the stream iteration loop
- No `await` between stream creation and `return new Response(stream, ...)`
  </verify>
  <done>
The chat API route uses ConverseStreamCommand for true streaming on the Bedrock fallback path, streams RLM responses in chunks, returns Response immediately with ReadableStream, checks abort signals during iteration, has maxDuration=60 for Vercel, and all existing features (auth, rate limiting, memory, search, learning) are preserved.
  </done>
</task>

</tasks>

<verification>
1. `npm run build` passes with no errors
2. `grep -n "ConverseStreamCommand" app/api/chat/route.ts` shows the streaming command is used
3. `grep -n "maxDuration" app/api/chat/route.ts` shows runtime config exists
4. `grep -n "request.signal.aborted" app/api/chat/route.ts` shows abort signal checking
5. `grep -n "return new Response(stream" app/api/chat/route.ts` shows immediate response return
</verification>

<success_criteria>
- Chat API returns streaming SSE responses (multiple `data:` lines, not one giant chunk)
- ConverseStreamCommand is used for Bedrock fallback (true token-by-token streaming)
- RLM responses are chunked into small pieces for consistent streaming UX
- Vercel maxDuration=60 is set for production timeout handling
- Abort signal is checked in the stream iteration loop
- All existing chat features still work (auth, rate limit, memory, search, learning, naming)
- `npm run build` passes
</success_criteria>

<output>
After completion, create `.planning/phases/09-streaming/09-01-SUMMARY.md`
</output>
