---
phase: 02-llm-integration
plan: 01
type: execute
---

<objective>
Deploy a self-hosted LLM on AWS SageMaker and connect it to the SoulPrint app.

Purpose: Replace Gemini with a self-hosted open-source model for personality mimicry. Get it working fast so we can test personality capture.
Output: Working AWS SageMaker endpoint with 13B model, callable from Next.js API route.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-phase.md
@~/.claude/get-shit-done/templates/summary.md
@~/.claude/get-shit-done/references/checkpoints.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-research/RESEARCH.md
@.planning/phases/02-llm-integration/02-CONTEXT.md

**From Research:**
- Use SageMaker with LMI (Large Model Inference) container
- Start with 13B model (MythoMax L2 or similar) on g5.xlarge (~$24/day)
- vLLM for efficient inference
- Full responses first, streaming later

**Existing:**
- Next.js 16 app with API routes
- Current Gemini integration at `app/api/gemini/chat/route.ts`
- Supabase for auth/database

**User note:** New AWS account - will need guided setup
</context>

<tasks>

<task type="checkpoint:human-action" gate="blocking">
  <action>Set up AWS credentials for SageMaker access</action>
  <instructions>
    You have a new AWS account. We need to:

    1. **Create IAM User for SageMaker:**
       - Go to AWS Console → IAM → Users → Create user
       - Name: `soulprint-sagemaker`
       - Attach policies: `AmazonSageMakerFullAccess`
       - Create access keys (CLI access)

    2. **Save credentials locally:**
       - Copy Access Key ID and Secret Access Key
       - I'll add them to your .env.local file

    3. **Request GPU quota (if needed):**
       - Go to Service Quotas → Amazon SageMaker
       - Search for "ml.g5.xlarge for endpoint usage"
       - Request increase to at least 1 (may take a few hours)

    Tell me when you have the Access Key ID and Secret Access Key ready.
  </instructions>
  <verification>User provides AWS credentials</verification>
  <resume-signal>Paste your Access Key ID and Secret Access Key (I'll add them to .env.local)</resume-signal>
</task>

<task type="auto">
  <name>Task 2: Configure AWS credentials and deploy model to SageMaker</name>
  <files>.env.local, lib/aws/sagemaker.ts</files>
  <action>
    1. Add AWS credentials to .env.local:
       ```
       AWS_ACCESS_KEY_ID=<from user>
       AWS_SECRET_ACCESS_KEY=<from user>
       AWS_REGION=us-east-1
       SAGEMAKER_ENDPOINT_NAME=soulprint-llm
       ```

    2. Create SageMaker deployment script at `lib/aws/sagemaker.ts`:
       - Use AWS SDK v3 (@aws-sdk/client-sagemaker-runtime)
       - Create model using LMI container with vLLM
       - Deploy HuggingFace model: `NousResearch/Hermes-2-Pro-Llama-3-8B` (good quality, fits g5.xlarge)
       - Configure endpoint with instance type `ml.g5.xlarge`

    3. Create deployment helper to deploy/delete endpoint on demand (for cost control)

    Note: Use Hermes-2-Pro-Llama-3-8B instead of MythoMax - better instruction following, same size, fits g5.xlarge easily.
  </action>
  <verify>TypeScript compiles without errors, AWS SDK imported correctly</verify>
  <done>AWS config in .env.local, sagemaker.ts client created with deploy/invoke functions</done>
</task>

<task type="auto">
  <name>Task 3: Create API route to call SageMaker endpoint</name>
  <files>app/api/llm/chat/route.ts</files>
  <action>
    Create POST endpoint at `/api/llm/chat` that:
    1. Accepts `{ messages: [{role, content}], temperature?, max_tokens? }`
    2. Calls SageMaker endpoint using the client from sagemaker.ts
    3. Returns full response (no streaming for now)
    4. Handles errors gracefully (endpoint not running, timeout, etc.)

    Format request for LMI container:
    ```json
    {
      "inputs": "<formatted prompt>",
      "parameters": {
        "max_new_tokens": 512,
        "temperature": 0.7,
        "do_sample": true
      }
    }
    ```

    Use standard chat format for prompt construction.
  </action>
  <verify>API route compiles, endpoint structure matches LMI container format</verify>
  <done>POST /api/llm/chat route created, ready to test once endpoint is deployed</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] AWS credentials in .env.local
- [ ] `npm run build` succeeds without errors
- [ ] SageMaker client code compiles
- [ ] API route created at /api/llm/chat
</verification>

<success_criteria>

- AWS credentials configured
- SageMaker deployment code ready
- API route created to call endpoint
- Code compiles without errors
- Ready to deploy model and test
</success_criteria>

<output>
After completion, create `.planning/phases/02-llm-integration/02-01-SUMMARY.md`

Note: Actual model deployment will happen during execution when user confirms AWS setup is ready. The endpoint takes ~10-15 minutes to spin up.
</output>
