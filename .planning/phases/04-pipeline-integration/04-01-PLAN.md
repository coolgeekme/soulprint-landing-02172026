---
phase: 04-pipeline-integration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - /home/drewpullen/clawd/soulprint-rlm/processors/full_pass.py
  - /home/drewpullen/clawd/soulprint-rlm/main.py
autonomous: true

must_haves:
  truths:
    - "FACT_EXTRACTION_CONCURRENCY env var controls parallel fact extraction (default 3)"
    - "Pipeline logs user_id and step name at every major boundary"
    - "full_pass_status transitions through processing -> complete/failed in user_profiles"
    - "Pipeline failure sets full_pass_error with step name and error message"
    - "V2 regeneration failure is non-fatal -- MEMORY saved, status still marked complete"
  artifacts:
    - path: "/home/drewpullen/clawd/soulprint-rlm/processors/full_pass.py"
      provides: "Pipeline orchestrator with configurable concurrency and step logging"
      contains: "FACT_EXTRACTION_CONCURRENCY"
    - path: "/home/drewpullen/clawd/soulprint-rlm/main.py"
      provides: "V2 background task with full_pass_status tracking"
      contains: "full_pass_status"
  key_links:
    - from: "/home/drewpullen/clawd/soulprint-rlm/processors/full_pass.py"
      to: "os.getenv('FACT_EXTRACTION_CONCURRENCY')"
      via: "get_concurrency_limit helper function"
      pattern: "FACT_EXTRACTION_CONCURRENCY"
    - from: "/home/drewpullen/clawd/soulprint-rlm/main.py"
      to: "adapters.supabase_adapter.update_user_profile"
      via: "full_pass_status updates in run_full_pass_v2_background"
      pattern: "full_pass_status.*processing|complete|failed"
---

<objective>
Harden the full pass pipeline with configurable concurrency, status tracking, and enhanced error logging.

Purpose: The pipeline exists (Phase 2-3) but has hardcoded concurrency=10 (will OOM on Render Starter), no status tracking in the v2 background task, and insufficient error context for debugging. This plan makes the pipeline production-ready by addressing all 6 Phase 4 requirements (PIPE-02, PIPE-03, PIPE-04, MON-01, MON-02, MON-03).

Output: Modified full_pass.py with configurable concurrency + step logging, modified main.py with full_pass_status tracking in run_full_pass_v2_background.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-pipeline-integration/04-RESEARCH.md
@.planning/phases/03-wire-new-endpoint/03-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Configurable concurrency + step logging in full_pass.py</name>
  <files>/home/drewpullen/clawd/soulprint-rlm/processors/full_pass.py</files>
  <action>
Modify `/home/drewpullen/clawd/soulprint-rlm/processors/full_pass.py`:

1. Add a `get_concurrency_limit()` helper function at top of file (after imports, before delete_user_chunks):
   - Read `FACT_EXTRACTION_CONCURRENCY` from `os.getenv()` inside the function body (per ADAPTER-01 pattern)
   - Default to 3 (conservative for Render Starter 512MB RAM)
   - Validate range: 1-50, log warning and return default if invalid
   - Handle ValueError for non-integer values

```python
def get_concurrency_limit() -> int:
    """Get fact extraction concurrency from environment (default 3 for Render Starter)."""
    default = 3
    try:
        raw = os.getenv("FACT_EXTRACTION_CONCURRENCY", str(default))
        limit = int(raw)
        if limit < 1 or limit > 50:
            print(f"[FullPass] WARN: Invalid concurrency {limit}, using default {default}")
            return default
        return limit
    except ValueError:
        print(f"[FullPass] WARN: Invalid FACT_EXTRACTION_CONCURRENCY value, using default {default}")
        return default
```

2. In `run_full_pass_pipeline()`, replace hardcoded `concurrency=10` on the `extract_facts_parallel` call (line ~108) with:
```python
concurrency = get_concurrency_limit()
all_facts = await extract_facts_parallel(chunks, client, concurrency=concurrency)
```

3. Add structured step-by-step logging throughout `run_full_pass_pipeline()`. At each major step boundary, log with format:
```python
print(f"[FullPass] user_id={user_id} step=download_conversations status=starting")
```
Then after completion:
```python
print(f"[FullPass] user_id={user_id} step=download_conversations conversations={len(conversations)}")
```

Steps to log (replace existing print statements):
- step=download_conversations (with conversation count)
- step=chunk_conversations (with chunk count)
- step=save_chunks (with chunk count)
- step=extract_facts (with concurrency limit and chunk count)
- step=consolidate_facts (with total_count)
- step=hierarchical_reduce (with token estimate)
- step=generate_memory (with memory length)
- step=save_memory
- step=v2_regeneration (with success/failure)
- step=complete

Keep the existing error handling structure. Do NOT change the function signature or return type. Do NOT change the import structure (still imports from adapters and processors modules).
  </action>
  <verify>
```bash
cd /home/drewpullen/clawd/soulprint-rlm && python3 -m py_compile processors/full_pass.py
```
Verify: no syntax errors.

```bash
grep -c "FACT_EXTRACTION_CONCURRENCY" /home/drewpullen/clawd/soulprint-rlm/processors/full_pass.py
```
Verify: at least 2 occurrences (function + docstring/comment).

```bash
grep -c "user_id=" /home/drewpullen/clawd/soulprint-rlm/processors/full_pass.py
```
Verify: at least 8 occurrences (one per step log).

```bash
grep "concurrency=10" /home/drewpullen/clawd/soulprint-rlm/processors/full_pass.py
```
Verify: 0 results (hardcoded value removed).
  </verify>
  <done>
get_concurrency_limit() reads FACT_EXTRACTION_CONCURRENCY env var (default 3, validated 1-50). All 9 pipeline steps log user_id and step name. Hardcoded concurrency=10 replaced with configurable value.
  </done>
</task>

<task type="auto">
  <name>Task 2: Status tracking + non-fatal failure in run_full_pass_v2_background</name>
  <files>/home/drewpullen/clawd/soulprint-rlm/main.py</files>
  <action>
Modify `/home/drewpullen/clawd/soulprint-rlm/main.py` -- specifically the `run_full_pass_v2_background` function (starts at line ~2528).

Replace the current simple try/except with full status tracking and non-fatal failure handling. The function currently:
- Calls run_full_pass_pipeline
- Calls complete_job on success/failure
- Logs with [v2] prefix

Update to:

1. At the START of the try block (before calling pipeline), update user_profiles:
```python
from adapters.supabase_adapter import update_user_profile as adapter_update_profile

# Mark pipeline as processing
await adapter_update_profile(user_id, {
    "full_pass_status": "processing",
    "full_pass_started_at": datetime.utcnow().isoformat(),
    "full_pass_error": None,
})
```

Use the adapter version `update_user_profile` (imported from adapters.supabase_adapter) rather than the main.py `update_user_profile` to avoid confusion. Import it with an alias like `adapter_update_profile` or import at the top of the function to avoid name collision with the existing `update_user_profile` in main.py.

NOTE: Both functions do the same thing (PATCH user_profiles via REST API). The adapter version reads env vars inside the function body (per ADAPTER-01). Either one works. Pick whichever avoids the name collision cleanly -- importing the adapter version inside the function body with an alias is cleanest:

```python
from adapters.supabase_adapter import update_user_profile as update_profile_status
```

2. On SUCCESS (after pipeline returns memory_md), update status:
```python
await update_profile_status(user_id, {
    "full_pass_status": "complete",
    "full_pass_completed_at": datetime.utcnow().isoformat(),
})
```

3. In the EXCEPT block, update status with error context including step info:
```python
error_msg = f"{type(e).__name__}: {str(e)}"
print(f"[v2] user_id={user_id} step=pipeline status=failed error={error_msg}")
import traceback
traceback.print_exc()

await update_profile_status(user_id, {
    "full_pass_status": "failed",
    "full_pass_error": f"Pipeline failed: {error_msg[:500]}",
})
```

4. Keep the existing `complete_job(job_id, ...)` calls. The job tracking and status tracking serve different purposes (job tracking is for recovery, status tracking is for user-facing state).

5. Ensure the `datetime` import is available. It's already imported at the top of main.py.

Do NOT change:
- The function signature
- The /process-full-v2 endpoint
- Any other functions in main.py
- The v1 /process-full or process_full_background functions
  </action>
  <verify>
```bash
cd /home/drewpullen/clawd/soulprint-rlm && python3 -m py_compile main.py
```
Verify: no syntax errors.

```bash
grep -c "full_pass_status" /home/drewpullen/clawd/soulprint-rlm/main.py
```
Verify: at least 3 occurrences (processing, complete, failed).

```bash
grep "full_pass_started_at\|full_pass_completed_at\|full_pass_error" /home/drewpullen/clawd/soulprint-rlm/main.py
```
Verify: all 3 fields present.

```bash
grep "adapters.supabase_adapter" /home/drewpullen/clawd/soulprint-rlm/main.py
```
Verify: adapter import present in run_full_pass_v2_background.
  </verify>
  <done>
run_full_pass_v2_background sets full_pass_status to "processing" at start with timestamp, "complete" on success with timestamp, "failed" on error with truncated error message. V2 regeneration failure in the pipeline itself is already non-fatal (full_pass.py returns memory_md even if v2_sections is None), so the v2 background task marks "complete" as long as MEMORY was generated. Pipeline-level exceptions (download, chunk, fact extraction failures) correctly mark "failed" with error context.
  </done>
</task>

</tasks>

<verification>
1. `cd /home/drewpullen/clawd/soulprint-rlm && python3 -m py_compile processors/full_pass.py && python3 -m py_compile main.py` -- both compile without errors
2. `grep "concurrency=10" /home/drewpullen/clawd/soulprint-rlm/processors/full_pass.py` -- returns 0 results (hardcoded value removed)
3. `grep "FACT_EXTRACTION_CONCURRENCY" /home/drewpullen/clawd/soulprint-rlm/processors/full_pass.py` -- returns 2+ results
4. `grep "full_pass_status" /home/drewpullen/clawd/soulprint-rlm/main.py` -- returns 3+ results (processing, complete, failed)
5. `cd /home/drewpullen/clawd/soulprint-rlm && source venv/bin/activate && python -m pytest tests/ -v --timeout=30` -- all existing 55 tests still pass (no regressions)
</verification>

<success_criteria>
- FACT_EXTRACTION_CONCURRENCY env var configures concurrency (default 3, validated 1-50)
- Hardcoded concurrency=10 eliminated from full_pass.py
- All pipeline steps log user_id and step name
- run_full_pass_v2_background tracks full_pass_status (processing/complete/failed) with timestamps
- Pipeline errors include step name and error type in full_pass_error field
- All 55 existing tests still pass
</success_criteria>

<output>
After completion, create `.planning/phases/04-pipeline-integration/04-01-SUMMARY.md`
</output>
