---
phase: 04-pipeline-integration
plan: 02
type: execute
wave: 2
depends_on: ["04-01"]
files_modified:
  - /home/drewpullen/clawd/soulprint-rlm/tests/test_full_pass_integration.py
  - /home/drewpullen/clawd/soulprint-rlm/tests/conftest.py
  - /home/drewpullen/clawd/soulprint-rlm/sql/20260207_full_pass_status.sql
autonomous: true

must_haves:
  truths:
    - "Pipeline integration test verifies all 9 steps execute in order with mocked APIs"
    - "Concurrency configuration test verifies FACT_EXTRACTION_CONCURRENCY env var works"
    - "Status tracking test verifies full_pass_status transitions (processing -> complete)"
    - "Non-fatal v2 failure test verifies pipeline completes even if v2 regeneration fails"
    - "SQL migration file exists for full_pass_status columns"
  artifacts:
    - path: "/home/drewpullen/clawd/soulprint-rlm/tests/test_full_pass_integration.py"
      provides: "Pipeline integration tests with mocked Anthropic and Supabase"
      min_lines: 80
    - path: "/home/drewpullen/clawd/soulprint-rlm/sql/20260207_full_pass_status.sql"
      provides: "SQL migration for full_pass_status columns"
      contains: "full_pass_status"
  key_links:
    - from: "/home/drewpullen/clawd/soulprint-rlm/tests/test_full_pass_integration.py"
      to: "processors.full_pass.get_concurrency_limit"
      via: "import and test with monkeypatch"
      pattern: "get_concurrency_limit"
    - from: "/home/drewpullen/clawd/soulprint-rlm/tests/test_full_pass_integration.py"
      to: "processors.full_pass.run_full_pass_pipeline"
      via: "import (may not call directly due to API deps)"
      pattern: "run_full_pass_pipeline"
---

<objective>
Write integration tests for the hardened pipeline and add the SQL migration file.

Purpose: The pipeline was hardened in Plan 01 but needs test coverage for the new concurrency configuration, step logging, and status tracking. Also need to ensure the SQL migration for full_pass_status columns exists in the soulprint-rlm repo (it already exists in soulprint-landing but not in soulprint-rlm). STATE.md explicitly notes "Processors not yet tested end-to-end with real Supabase data. Phase 4 should include smoke test."

Output: Integration test file covering concurrency config + pipeline step verification + status tracking, SQL migration file.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-pipeline-integration/04-RESEARCH.md
@.planning/phases/04-pipeline-integration/04-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Pipeline integration tests</name>
  <files>/home/drewpullen/clawd/soulprint-rlm/tests/test_full_pass_integration.py, /home/drewpullen/clawd/soulprint-rlm/tests/conftest.py</files>
  <action>
Create `/home/drewpullen/clawd/soulprint-rlm/tests/test_full_pass_integration.py` with the following tests. Use the existing test patterns from test_processors.py and conftest.py (monkeypatched env vars, pytest-asyncio).

**Test 1: get_concurrency_limit defaults to 3**
```python
def test_get_concurrency_limit_default():
    """Default concurrency is 3 for Render Starter tier."""
    from processors.full_pass import get_concurrency_limit
    # conftest.py doesn't set FACT_EXTRACTION_CONCURRENCY, so default applies
    assert get_concurrency_limit() == 3
```

**Test 2: get_concurrency_limit reads env var**
```python
def test_get_concurrency_limit_from_env(monkeypatch):
    """Concurrency reads from FACT_EXTRACTION_CONCURRENCY env var."""
    from processors.full_pass import get_concurrency_limit
    monkeypatch.setenv("FACT_EXTRACTION_CONCURRENCY", "5")
    assert get_concurrency_limit() == 5
```

**Test 3: get_concurrency_limit rejects invalid values**
```python
def test_get_concurrency_limit_invalid_returns_default(monkeypatch):
    """Invalid concurrency values fall back to default 3."""
    from processors.full_pass import get_concurrency_limit

    # Non-integer
    monkeypatch.setenv("FACT_EXTRACTION_CONCURRENCY", "abc")
    assert get_concurrency_limit() == 3

    # Too high
    monkeypatch.setenv("FACT_EXTRACTION_CONCURRENCY", "100")
    assert get_concurrency_limit() == 3

    # Zero
    monkeypatch.setenv("FACT_EXTRACTION_CONCURRENCY", "0")
    assert get_concurrency_limit() == 3

    # Negative
    monkeypatch.setenv("FACT_EXTRACTION_CONCURRENCY", "-1")
    assert get_concurrency_limit() == 3
```

**Test 4: get_concurrency_limit accepts boundary values**
```python
def test_get_concurrency_limit_boundary_values(monkeypatch):
    """Concurrency accepts valid boundary values 1 and 50."""
    from processors.full_pass import get_concurrency_limit

    monkeypatch.setenv("FACT_EXTRACTION_CONCURRENCY", "1")
    assert get_concurrency_limit() == 1

    monkeypatch.setenv("FACT_EXTRACTION_CONCURRENCY", "50")
    assert get_concurrency_limit() == 50
```

**Test 5: Pipeline step logging includes user_id** (verify the logging format by capturing stdout)
```python
@pytest.mark.asyncio
async def test_pipeline_logs_user_id(monkeypatch, capsys):
    """Pipeline logs user_id at each step boundary."""
    from processors.full_pass import run_full_pass_pipeline

    # Mock all external dependencies
    async def mock_download(path):
        return [{"id": "conv-1", "title": "Test", "messages": [{"role": "user", "content": "Hi"}]}]

    async def mock_update(user_id, updates):
        pass

    async def mock_save_chunks(user_id, chunks):
        pass

    # Mock adapter functions
    monkeypatch.setattr("processors.full_pass.download_conversations", mock_download)
    monkeypatch.setattr("processors.full_pass.update_user_profile", mock_update)
    monkeypatch.setattr("processors.full_pass.save_chunks_batch", mock_save_chunks)

    # Mock Anthropic client that returns valid JSON for fact extraction and memory
    class MockMessage:
        def __init__(self, text):
            self.text = text

    class MockResponse:
        def __init__(self, text):
            self.content = [MockMessage(text)]

    class MockMessages:
        async def create(self, **kwargs):
            prompt_text = kwargs.get("messages", [{}])[0].get("content", "")
            if "Extract ONLY factual" in prompt_text:
                return MockResponse('{"preferences": ["test"], "projects": [], "dates": [], "beliefs": [], "decisions": []}')
            elif "MEMORY section" in prompt_text or "creating a MEMORY" in prompt_text:
                return MockResponse("## Preferences\n- Test preference\n\n## Projects\nNo data yet.")
            elif "personality profile" in prompt_text or "Generate EXACTLY" in prompt_text:
                # V2 regeneration - return None-equivalent to test non-fatal
                return MockResponse('{"soul": {}, "identity": {}, "user": {}, "agents": {}, "tools": {}}')
            else:
                return MockResponse("{}")

    class MockAnthropicClient:
        messages = MockMessages()

    # Patch the Anthropic client creation
    monkeypatch.setattr("anthropic.AsyncAnthropic", lambda **kwargs: MockAnthropicClient())

    # Run pipeline
    memory_md = await run_full_pass_pipeline(
        user_id="test-user-123",
        storage_path="test-exports/test.json",
        conversation_count=1,
    )

    # Verify logging
    captured = capsys.readouterr()
    assert "user_id=test-user-123" in captured.out
    assert "step=download_conversations" in captured.out
    assert "step=chunk_conversations" in captured.out
    assert "step=extract_facts" in captured.out

    # Verify MEMORY was generated
    assert memory_md is not None
    assert len(memory_md) > 0
```

NOTE: The mock structure for the Anthropic client may need adjustment based on how the pipeline calls it. The key test is that user_id appears in stdout logs. If the mock setup is too complex for the pipeline's internal import pattern (lazy imports from processors.*), simplify by only testing get_concurrency_limit and leaving the pipeline logging as a verification step (grep the source code for the logging pattern).

If the full pipeline mock is too brittle (due to lazy imports from `processors.conversation_chunker`, `processors.fact_extractor`, etc.), replace Test 5 with a simpler source-code verification test:

```python
def test_pipeline_has_user_id_logging():
    """Pipeline source code includes user_id in step logging."""
    import inspect
    from processors.full_pass import run_full_pass_pipeline
    source = inspect.getsource(run_full_pass_pipeline)
    assert "user_id=" in source
    assert "step=" in source
```

Also add to conftest.py if not already present: ensure FACT_EXTRACTION_CONCURRENCY is NOT set (so default applies):
```python
# In the existing mock_env_vars fixture, do NOT set FACT_EXTRACTION_CONCURRENCY
# This ensures get_concurrency_limit() returns default (3)
```
Verify the existing conftest.py doesn't set it. If it does, remove it.
  </action>
  <verify>
```bash
cd /home/drewpullen/clawd/soulprint-rlm && source venv/bin/activate && python -m pytest tests/test_full_pass_integration.py -v --timeout=30
```
Verify: all new tests pass.

```bash
cd /home/drewpullen/clawd/soulprint-rlm && source venv/bin/activate && python -m pytest tests/ -v --timeout=60
```
Verify: all tests pass (55 existing + new integration tests, no regressions).
  </verify>
  <done>
Integration tests verify: get_concurrency_limit returns 3 by default, reads from env var, rejects invalid values, accepts boundary values. Pipeline logging verified to include user_id and step names. All existing tests still pass.
  </done>
</task>

<task type="auto">
  <name>Task 2: SQL migration file for full_pass_status columns</name>
  <files>/home/drewpullen/clawd/soulprint-rlm/sql/20260207_full_pass_status.sql</files>
  <action>
Create `/home/drewpullen/clawd/soulprint-rlm/sql/20260207_full_pass_status.sql` with the same content as the existing migration in soulprint-landing (`supabase/migrations/20260207_full_pass_schema.sql`).

The SQL should add these columns to user_profiles (all idempotent with IF NOT EXISTS):
- `full_pass_status TEXT DEFAULT 'pending'` with CHECK constraint for valid values (pending, processing, complete, failed)
- `full_pass_started_at TIMESTAMPTZ`
- `full_pass_completed_at TIMESTAMPTZ`
- `full_pass_error TEXT`
- `memory_md TEXT` (for the MEMORY section content)

Also add comments on each column explaining its purpose.

Header comment should note:
- This migration is required BEFORE deploying the pipeline hardening changes
- Must be executed in Supabase SQL Editor
- Can be re-run safely (all operations use IF NOT EXISTS / ADD CONSTRAINT ... CHECK)

NOTE: This migration may already be deployed (it existed in soulprint-landing from v1.2). The IF NOT EXISTS pattern ensures idempotency.
  </action>
  <verify>
```bash
test -f /home/drewpullen/clawd/soulprint-rlm/sql/20260207_full_pass_status.sql && echo "EXISTS" || echo "MISSING"
```
Verify: "EXISTS"

```bash
grep "full_pass_status" /home/drewpullen/clawd/soulprint-rlm/sql/20260207_full_pass_status.sql
```
Verify: at least 2 occurrences (column definition + CHECK constraint).

```bash
grep "IF NOT EXISTS" /home/drewpullen/clawd/soulprint-rlm/sql/20260207_full_pass_status.sql
```
Verify: at least 4 occurrences (one per column).
  </verify>
  <done>
SQL migration file exists in soulprint-rlm repo with idempotent schema for full_pass_status tracking (status, timestamps, error, memory_md columns). Ready for execution in Supabase SQL Editor if not already applied.
  </done>
</task>

</tasks>

<verification>
1. `cd /home/drewpullen/clawd/soulprint-rlm && source venv/bin/activate && python -m pytest tests/ -v --timeout=60` -- all tests pass (existing 55 + new integration tests)
2. `test -f /home/drewpullen/clawd/soulprint-rlm/sql/20260207_full_pass_status.sql` -- SQL migration exists
3. `grep "full_pass_status" /home/drewpullen/clawd/soulprint-rlm/sql/20260207_full_pass_status.sql` -- contains status column definition
4. `grep "get_concurrency_limit" /home/drewpullen/clawd/soulprint-rlm/tests/test_full_pass_integration.py` -- tests reference the new function
</verification>

<success_criteria>
- All new integration tests pass
- All existing 55 tests still pass (no regressions)
- SQL migration file exists with idempotent schema
- get_concurrency_limit tested with default, env var, invalid values, and boundary values
- Pipeline logging format verified (user_id + step in source code)
</success_criteria>

<output>
After completion, create `.planning/phases/04-pipeline-integration/04-02-SUMMARY.md`
</output>
