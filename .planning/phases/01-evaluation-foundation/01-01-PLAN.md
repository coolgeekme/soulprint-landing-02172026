---
phase: 01-evaluation-foundation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - lib/evaluation/datasets.ts
  - lib/evaluation/judges.ts
  - lib/evaluation/types.ts
autonomous: true
user_setup:
  - service: opik
    why: "LLM evaluation and experiment tracking platform"
    env_vars:
      - name: OPIK_API_KEY
        source: "Sign up at https://www.comet.com/opik → Settings → API Keys"
      - name: OPIK_WORKSPACE_NAME
        source: "Opik workspace name (visible in dashboard URL)"

must_haves:
  truths:
    - "Anonymized evaluation datasets can be created from chat_messages table"
    - "Three custom LLM-as-judge scorers exist for personality consistency, factuality, and tone matching"
    - "Judge scorers use Haiku 4.5 (different model family than Sonnet 4.5 generation) to avoid self-preference bias"
    - "Dataset items include user message, assistant response, soulprint context, and anonymized metadata"
  artifacts:
    - path: "lib/evaluation/types.ts"
      provides: "Shared types for evaluation infrastructure"
      contains: "ChatEvalItem"
    - path: "lib/evaluation/datasets.ts"
      provides: "Dataset creation from chat_messages with SHA256 anonymization"
      exports: ["createEvaluationDataset"]
    - path: "lib/evaluation/judges.ts"
      provides: "Three custom LLM-as-judge scorers extending Opik BaseMetric"
      exports: ["PersonalityConsistencyJudge", "FactualityJudge", "ToneMatchingJudge"]
  key_links:
    - from: "lib/evaluation/datasets.ts"
      to: "lib/supabase/server"
      via: "getSupabaseAdmin for chat_messages query"
      pattern: "getSupabaseAdmin"
    - from: "lib/evaluation/datasets.ts"
      to: "lib/opik"
      via: "getOpikClient for dataset creation"
      pattern: "getOpikClient"
    - from: "lib/evaluation/judges.ts"
      to: "lib/bedrock"
      via: "bedrockChatJSON with HAIKU_45 for judge scoring"
      pattern: "bedrockChatJSON.*HAIKU_45"
---

<objective>
Create the core evaluation library: dataset extraction from production chat data and three custom LLM-as-judge scoring metrics.

Purpose: Establish the data and scoring foundation that the experiment runner (Plan 02) will use. Datasets provide the evaluation inputs, judges provide the scoring functions. Together they satisfy EVAL-01 (datasets exist) and EVAL-02 (judge rubrics exist).

Output: `lib/evaluation/types.ts`, `lib/evaluation/datasets.ts`, `lib/evaluation/judges.ts`
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

@lib/opik.ts
@lib/bedrock.ts
@lib/soulprint/types.ts
@lib/soulprint/prompt-helpers.ts
@app/api/chat/route.ts
@.planning/phases/01-evaluation-foundation/01-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create evaluation types and dataset extraction</name>
  <files>lib/evaluation/types.ts, lib/evaluation/datasets.ts</files>
  <action>
Create `lib/evaluation/types.ts` with shared types:

```typescript
export interface ChatEvalItem {
  user_message: string;
  assistant_response: string;
  soulprint_context: {
    soul: Record<string, unknown> | null;
    identity: Record<string, unknown> | null;
    user: Record<string, unknown> | null;
    agents: Record<string, unknown> | null;
    tools: Record<string, unknown> | null;
  };
  expected_traits: string[];       // From soul.personality_traits
  expected_tone: string;           // From soul.tone_preferences
  expected_style: string;          // From agents.response_style
  metadata: {
    conversation_id: string;
    message_pair_id: string;
    user_id_hash: string;          // SHA256 anonymized
    extracted_at: string;           // ISO date
  };
}
```

Create `lib/evaluation/datasets.ts` with `createEvaluationDataset(limit?: number)`:

1. Import `getSupabaseAdmin` from `@/lib/supabase/server` and `getOpikClient` from `@/lib/opik`
2. Use `createHash('sha256')` from Node `crypto` for user ID anonymization
3. Query `chat_messages` table ordered by `created_at DESC`, limit `limit * 2` (need pairs)
4. Query `user_profiles` for matching user_ids to get soulprint sections (`soul_md`, `identity_md`, `user_md`, `agents_md`, `tools_md`)
5. Pair consecutive user/assistant messages (skip if roles don't alternate correctly)
6. Parse section JSONs safely with try/catch (return null on parse failure)
7. Extract `expected_traits` from parsed `soul_md.personality_traits` (default to empty array)
8. Extract `expected_tone` from parsed `soul_md.tone_preferences` (default to empty string)
9. Extract `expected_style` from parsed `agents_md.response_style` (default to empty string)
10. Create dataset via `opik.getOrCreateDataset<ChatEvalItem>(datasetName, description)` where datasetName includes date: `chat-eval-${YYYY-MM-DD}`
11. Insert items with `dataset.insert(items)`
12. Return `{ datasetName, itemCount }` for caller reporting
13. Require minimum 10 valid pairs or throw descriptive error

Important: All user IDs must be SHA256 hashed. No raw PII in dataset items. If OPIK_API_KEY is not set, throw a clear error message.
  </action>
  <verify>
    Run `npx tsc --noEmit lib/evaluation/types.ts lib/evaluation/datasets.ts` — no type errors.
    Verify imports resolve: `getSupabaseAdmin`, `getOpikClient`, `createHash`.
    Verify `ChatEvalItem` type is exported and used in datasets.ts.
  </verify>
  <done>
    `lib/evaluation/types.ts` exports `ChatEvalItem` type.
    `lib/evaluation/datasets.ts` exports `createEvaluationDataset` function that queries chat_messages, pairs user/assistant messages, anonymizes user IDs with SHA256, enriches with soulprint sections, and creates an Opik dataset.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create three custom LLM-as-judge scoring metrics</name>
  <files>lib/evaluation/judges.ts</files>
  <action>
Create `lib/evaluation/judges.ts` with three judge classes that extend Opik's `BaseMetric`:

**All three judges share these patterns:**
- Extend `BaseMetric` from `opik`
- Use `bedrockChatJSON` from `@/lib/bedrock` with model `'HAIKU_45'` and temperature `0.1`
- Include explicit anti-length-bias instruction: "Do NOT favor longer responses. Judge based on quality, not quantity. Conciseness is a virtue."
- Return `EvaluationScoreResult` with `{ name, value (0.0-1.0), reason }`
- Each has a `validationSchema` using `z.object()` from `opik` (re-exported zod)

**1. PersonalityConsistencyJudge**
- Name: `personality_consistency`
- Schema: `{ input: z.string(), output: z.string(), expected_traits: z.array(z.string()), soulprint_context: z.record(z.unknown()).optional() }`
- Judge prompt evaluates whether the assistant response matches expected personality traits:
  - Communication style alignment
  - Tone preference adherence
  - Personality trait reflection
  - Boundary respect
- Score 0.0-1.0

**2. FactualityJudge**
- Name: `factuality`
- Schema: `{ input: z.string(), output: z.string(), context: z.array(z.string()).optional() }`
- Judge prompt evaluates whether the assistant response:
  - Contains only claims supported by the conversation context or general knowledge
  - Does not hallucinate specific facts about the user
  - Acknowledges uncertainty when appropriate
  - Does not fabricate memories or personal details
- Score 0.0-1.0

**3. ToneMatchingJudge**
- Name: `tone_matching`
- Schema: `{ input: z.string(), output: z.string(), expected_tone: z.string(), expected_style: z.string() }`
- Judge prompt evaluates whether the assistant response:
  - Matches the expected tone (casual, formal, etc.)
  - Follows the expected response style (concise, detailed, etc.)
  - Uses appropriate formality level
  - Avoids chatbot-like patterns (excessive greetings, disclaimers)
- Score 0.0-1.0

**Implementation detail for extending BaseMetric:**
The `score` method receives `unknown` input. Validate it against `validationSchema` using `.safeParse()`, return `{ name, value: 0, reason: 'Invalid input', scoringFailed: true }` on validation failure. On success, call `bedrockChatJSON` with the judge prompt and parse the response.

The `bedrockChatJSON` response type should be `{ score: number; reasoning: string }`. If the LLM returns a score outside 0.0-1.0, clamp it. If JSON parsing fails, return scoringFailed: true.
  </action>
  <verify>
    Run `npx tsc --noEmit lib/evaluation/judges.ts` — no type errors.
    Verify all three classes are exported.
    Verify each class extends `BaseMetric` and has `validationSchema` and `score` method.
    Verify `bedrockChatJSON` is called with `model: 'HAIKU_45'` and `temperature: 0.1`.
  </verify>
  <done>
    `lib/evaluation/judges.ts` exports `PersonalityConsistencyJudge`, `FactualityJudge`, and `ToneMatchingJudge`.
    Each extends `BaseMetric`, validates input with Zod, calls Haiku 4.5 as judge with anti-length-bias instructions, and returns `EvaluationScoreResult` with score 0.0-1.0 and reasoning.
  </done>
</task>

</tasks>

<verification>
1. `npx tsc --noEmit` passes with no errors in `lib/evaluation/` files
2. All three files exist: `types.ts`, `datasets.ts`, `judges.ts`
3. `datasets.ts` uses SHA256 hashing (grep for `createHash.*sha256`)
4. `judges.ts` uses HAIKU_45 model (grep for `HAIKU_45`)
5. `judges.ts` includes anti-length-bias instruction (grep for "Do NOT favor longer")
6. No raw user IDs appear in dataset item construction (no `user_id` without hashing)
</verification>

<success_criteria>
- ChatEvalItem type defined with soulprint context, expected traits/tone/style, and anonymized metadata
- createEvaluationDataset function creates Opik dataset from chat_messages with SHA256 anonymization
- Three judge classes extend BaseMetric with validated schemas and Haiku 4.5 scoring
- All types compile without errors
- Requirements EVAL-01 (datasets) and EVAL-02 (judge rubrics) are satisfied
- NOTE: Phase 1 Success Criterion #5 (Opik latency <100ms P95) is deferred to Phase 5 (Integration Validation), which explicitly covers latency benchmarking under load. Not covered by Phase 1 plans.
</success_criteria>

<output>
After completion, create `.planning/phases/01-evaluation-foundation/01-01-SUMMARY.md`
</output>
