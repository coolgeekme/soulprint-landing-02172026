---
phase: 01-evaluation-foundation
plan: 02
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - lib/evaluation/experiments.ts
  - lib/evaluation/baseline.ts
  - scripts/create-eval-dataset.ts
  - scripts/run-experiment.ts
  - scripts/record-baseline.ts
autonomous: true

must_haves:
  truths:
    - "Developer can create evaluation datasets by running a CLI script"
    - "Developer can run offline experiments comparing prompt variants with aggregate scores"
    - "Baseline metrics are recorded for current v1 prompt system"
    - "Experiment results include per-item scores and aggregate statistics"
    - "CLI scripts provide clear progress output and error messages"
  artifacts:
    - path: "lib/evaluation/experiments.ts"
      provides: "Experiment runner that evaluates prompt variants against datasets"
      exports: ["runExperiment"]
    - path: "lib/evaluation/baseline.ts"
      provides: "Baseline metric recording using current v1 prompts"
      exports: ["recordBaseline"]
    - path: "scripts/create-eval-dataset.ts"
      provides: "CLI script to extract chat data into Opik dataset"
      min_lines: 20
    - path: "scripts/run-experiment.ts"
      provides: "CLI script to run offline experiment with prompt variants"
      min_lines: 20
    - path: "scripts/record-baseline.ts"
      provides: "CLI script to record v1 baseline metrics"
      min_lines: 20
  key_links:
    - from: "lib/evaluation/experiments.ts"
      to: "lib/evaluation/judges.ts"
      via: "imports judge classes as scoring metrics"
      pattern: "PersonalityConsistencyJudge|FactualityJudge|ToneMatchingJudge"
    - from: "lib/evaluation/experiments.ts"
      to: "opik"
      via: "Opik evaluate() function for experiment orchestration"
      pattern: "evaluate"
    - from: "lib/evaluation/baseline.ts"
      to: "lib/evaluation/experiments.ts"
      via: "uses runExperiment with v1 prompt builder"
      pattern: "runExperiment"
    - from: "lib/evaluation/baseline.ts"
      to: "app/api/chat/route.ts"
      via: "reuses buildSystemPrompt logic for v1 prompt generation"
      pattern: "buildSystemPrompt|formatSection|cleanSection"
    - from: "scripts/create-eval-dataset.ts"
      to: "lib/evaluation/datasets.ts"
      via: "calls createEvaluationDataset"
      pattern: "createEvaluationDataset"
    - from: "scripts/record-baseline.ts"
      to: "lib/evaluation/baseline.ts"
      via: "calls recordBaseline"
      pattern: "recordBaseline"
---

<objective>
Build the experiment runner, baseline recording, and CLI scripts that use the evaluation library from Plan 01.

Purpose: Enable developers to run offline experiments comparing prompt variants (EVAL-03) and record baseline metrics for the current v1 prompt system (EVAL-04). CLI scripts make this a repeatable, documented process.

Output: `lib/evaluation/experiments.ts`, `lib/evaluation/baseline.ts`, `scripts/create-eval-dataset.ts`, `scripts/run-experiment.ts`, `scripts/record-baseline.ts`
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

@lib/opik.ts
@lib/bedrock.ts
@lib/soulprint/prompt-helpers.ts
@lib/soulprint/prompts.ts
@app/api/chat/route.ts
@.planning/phases/01-evaluation-foundation/01-RESEARCH.md
@.planning/phases/01-evaluation-foundation/01-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create experiment runner and baseline recording</name>
  <files>lib/evaluation/experiments.ts, lib/evaluation/baseline.ts</files>
  <action>
**Create `lib/evaluation/experiments.ts`:**

Export `runExperiment` function:

```typescript
interface PromptVariant {
  name: string;
  buildSystemPrompt: (item: ChatEvalItem) => string;
}

interface ExperimentResult {
  experimentId: string;
  experimentName: string;
  resultUrl?: string;
  aggregateScores: Record<string, { mean: number; min: number; max: number; count: number }>;
}

async function runExperiment(options: {
  datasetName: string;
  variant: PromptVariant;
  experimentName?: string;
  nbSamples?: number;
}): Promise<ExperimentResult>
```

Implementation:
1. Import `evaluate` from `opik` and `getOpikClient` from `@/lib/opik`
2. Import `bedrockChat` from `@/lib/bedrock` for generating responses with the variant's prompt
3. Import all three judges from `@/lib/evaluation/judges`
4. Get dataset via `opik.getDataset<ChatEvalItem>(datasetName)`
5. Create task function: for each dataset item, build system prompt via `variant.buildSystemPrompt(item)`, call `bedrockChat` with model `'HAIKU_45'` (use Haiku for cost-effective offline evaluation, not Sonnet), pass `item.user_message` as the user message, return `{ output: response, input: item.user_message, expected_traits: item.expected_traits, expected_tone: item.expected_tone, expected_style: item.expected_style, soulprint_context: item.soulprint_context }`
6. Call `evaluate({ dataset, task, scoringMetrics: [new PersonalityConsistencyJudge(), new FactualityJudge(), new ToneMatchingJudge()], experimentName, nbSamples, client: opik })`
7. Compute aggregate scores from `result.testResults` — for each metric name, calculate mean/min/max/count across all items
8. Return `ExperimentResult` with aggregateScores

**Create `lib/evaluation/baseline.ts`:**

Export `recordBaseline` function:

```typescript
interface BaselineResult extends ExperimentResult {
  version: string;
  recordedAt: string;
}

async function recordBaseline(options: {
  datasetName: string;
  nbSamples?: number;
}): Promise<BaselineResult>
```

Implementation:
1. Import `runExperiment` from `./experiments`
2. Import `cleanSection`, `formatSection` from `@/lib/soulprint/prompt-helpers`
3. Create v1 prompt builder that replicates the current `buildSystemPrompt` logic from `app/api/chat/route.ts`:
   - Accept a `ChatEvalItem` and build the OpenClaw-style system prompt
   - Use `formatSection` for SOUL, IDENTITY, USER, AGENTS, TOOLS sections from `item.soulprint_context`
   - Use `cleanSection` to filter placeholders before formatting
   - Include the same preamble text as the production chat route: "You have memories of this person...", "Be direct. Have opinions...", "NEVER start responses with greetings..."
   - Use AI name "SoulPrint" (default) since eval doesn't have per-user names
4. Call `runExperiment({ datasetName, variant: { name: 'v1-baseline', buildSystemPrompt: v1Builder }, experimentName: 'baseline-v1-prompts', nbSamples })`
5. Return result with `version: 'v1'` and `recordedAt: new Date().toISOString()`
6. Log aggregate scores to console in a readable table format
  </action>
  <verify>
    Run `npx tsc --noEmit lib/evaluation/experiments.ts lib/evaluation/baseline.ts` — no type errors.
    Verify `runExperiment` uses `evaluate` from opik.
    Verify `recordBaseline` uses `formatSection` and `cleanSection` from prompt-helpers.
    Verify the v1 prompt builder matches the production `buildSystemPrompt` pattern from chat route.
  </verify>
  <done>
    `lib/evaluation/experiments.ts` exports `runExperiment` that evaluates a prompt variant against an Opik dataset using three custom judges and returns aggregate scores.
    `lib/evaluation/baseline.ts` exports `recordBaseline` that runs an experiment with the current v1 prompt builder and records it as the baseline.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create CLI scripts for dataset creation, experiments, and baseline</name>
  <files>scripts/create-eval-dataset.ts, scripts/run-experiment.ts, scripts/record-baseline.ts</files>
  <action>
All scripts use `tsx` for execution (no compilation needed). Each script:
- Imports `dotenv/config` at the top (or uses `--env-file .env.local` flag)
- Has a main() async function wrapped in try/catch
- Provides clear console output with progress messages
- Exits with code 1 on error, 0 on success
- Accepts command-line arguments via `process.argv` for key parameters

**NOTE on module resolution:** These scripts run with `tsx` which handles `@/` path aliases from tsconfig. Verify by checking that `tsx` is available: `npx tsx --version`. If not, use relative imports as fallback.

**1. `scripts/create-eval-dataset.ts`:**

```
Usage: npx tsx scripts/create-eval-dataset.ts [--limit N]
Default limit: 100
```

- Import and call `createEvaluationDataset(limit)` from `@/lib/evaluation/datasets`
- Parse `--limit` from argv (default 100)
- Print: "Creating evaluation dataset..." → "Dataset '{name}' created with {N} items" or error message
- Validate limit >= 20 (minimum for meaningful evaluation)

**2. `scripts/run-experiment.ts`:**

```
Usage: npx tsx scripts/run-experiment.ts --dataset <name> --variant <name> [--samples N]
```

- Import `runExperiment` from `@/lib/evaluation/experiments`
- Parse `--dataset`, `--variant`, `--samples` from argv
- For now, only support `--variant v1` (current prompt system). Later phases will add v2 variants.
- Build a simple variant mapping: `{ v1: v1PromptBuilder }` imported from baseline.ts
- Print experiment results as a formatted table showing per-metric aggregate scores
- Print the Opik dashboard URL if available

**3. `scripts/record-baseline.ts`:**

```
Usage: npx tsx scripts/record-baseline.ts --dataset <name> [--samples N]
```

- Import and call `recordBaseline({ datasetName, nbSamples })` from `@/lib/evaluation/baseline`
- Parse `--dataset` and `--samples` from argv
- Print baseline results as formatted table:
  ```
  === V1 BASELINE METRICS ===
  Personality Consistency: 0.XX (min: 0.XX, max: 0.XX, n=NN)
  Factuality:             0.XX (min: 0.XX, max: 0.XX, n=NN)
  Tone Matching:          0.XX (min: 0.XX, max: 0.XX, n=NN)

  Experiment ID: xxx
  Dashboard URL: https://...
  Recorded at: 2026-02-08T...
  ```

**Environment loading:** Each script should load `.env.local` for Supabase, Bedrock, and Opik credentials. Use `import 'dotenv/config'` with `DOTENV_CONFIG_PATH=.env.local` or use the tsx `--env-file` flag. Document the correct invocation in a comment at the top of each script.
  </action>
  <verify>
    Run `npx tsc --noEmit scripts/create-eval-dataset.ts scripts/run-experiment.ts scripts/record-baseline.ts` — no type errors (or use tsx for checking if tsc has path issues with scripts dir).
    Verify each script has a clear usage comment at the top.
    Verify each script handles missing arguments with helpful error messages.
    Verify `create-eval-dataset.ts` calls `createEvaluationDataset`.
    Verify `record-baseline.ts` calls `recordBaseline`.
  </verify>
  <done>
    Three CLI scripts exist in `scripts/` directory:
    - `create-eval-dataset.ts` creates an Opik dataset from production chat data
    - `run-experiment.ts` runs an experiment with a prompt variant against a dataset
    - `record-baseline.ts` records v1 baseline metrics
    All scripts provide clear usage instructions, progress output, and error handling.
  </done>
</task>

</tasks>

<verification>
1. `npx tsc --noEmit` passes for all files in `lib/evaluation/` and `scripts/`
2. All 5 new files exist: `experiments.ts`, `baseline.ts`, `create-eval-dataset.ts`, `run-experiment.ts`, `record-baseline.ts`
3. `experiments.ts` imports `evaluate` from `opik` (grep for `from 'opik'`)
4. `baseline.ts` replicates v1 prompt pattern (grep for `formatSection`)
5. Scripts handle missing args gracefully (each has argv parsing with defaults)
6. No hardcoded credentials in any file (grep for API keys)
</verification>

<success_criteria>
- runExperiment function evaluates prompt variants against Opik datasets using all three judges
- recordBaseline function captures v1 prompt system metrics as named experiment
- Three CLI scripts provide ergonomic developer workflow for creating datasets, running experiments, and recording baselines
- Requirements EVAL-03 (experiment runner) and EVAL-04 (baseline metrics) are satisfied
- Complete evaluation pipeline is runnable: create-dataset → record-baseline → compare variants
</success_criteria>

<output>
After completion, create `.planning/phases/01-evaluation-foundation/01-02-SUMMARY.md`
</output>
