---
phase: 02-parsing-quality
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - rlm-service/processors/dag_parser.py
  - rlm-service/processors/streaming_import.py
  - rlm-service/processors/conversation_chunker.py
  - rlm-service/processors/test_dag_parser.py
  - ~/clawd/soulprint-rlm/processors/dag_parser.py
  - ~/clawd/soulprint-rlm/processors/streaming_import.py
  - ~/clawd/soulprint-rlm/processors/conversation_chunker.py
autonomous: true

must_haves:
  truths:
    - "Conversations with edits/regenerations produce only the active message path (no dead branches)"
    - "Tool outputs, browsing traces, and system messages are excluded from parsed output"
    - "Messages with multiple content.parts have all text parts extracted (not just parts[0])"
    - "Both bare array [...] and wrapped { conversations: [...] } formats parse successfully"
    - "Conversations missing current_node still parse via fallback traversal"
  artifacts:
    - path: "rlm-service/processors/dag_parser.py"
      provides: "DAG traversal, message filtering, content extraction helpers"
      exports: ["extract_active_path", "is_visible_message", "extract_content"]
      min_lines: 60
    - path: "rlm-service/processors/test_dag_parser.py"
      provides: "Verification tests for DAG parser functions"
      min_lines: 80
  key_links:
    - from: "rlm-service/processors/streaming_import.py"
      to: "rlm-service/processors/dag_parser.py"
      via: "import extract_active_path, is_visible_message, extract_content"
      pattern: "from \\.dag_parser import"
    - from: "rlm-service/processors/conversation_chunker.py"
      to: "rlm-service/processors/dag_parser.py"
      via: "import extract_active_path, is_visible_message, extract_content"
      pattern: "from \\.dag_parser import"
    - from: "rlm-service/processors/streaming_import.py"
      to: "rlm-service/processors/quick_pass.py"
      via: "parsed conversations with messages key"
      pattern: "\"messages\": parsed_messages"
---

<objective>
Implement DAG traversal, hidden message filtering, and polymorphic content.parts handling for ChatGPT export parsing.

Purpose: Current parsing traverses forward from root through children, capturing dead branches from edits/regenerations. It also includes tool outputs and only reads parts[0]. This produces noisy, inaccurate soulprints. DAG traversal from current_node through parent chain extracts only the active conversation path, producing clean input for personality analysis.

Output: Shared parsing helpers used by both streaming_import.py and conversation_chunker.py, with test coverage proving correctness against known ChatGPT export structures.
</objective>

<execution_context>
@./.claude/agents/gsd-planner.md
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-parsing-quality/02-RESEARCH.md
@rlm-service/processors/streaming_import.py
@rlm-service/processors/conversation_chunker.py
@rlm-service/processors/sample.py
@rlm-service/processors/quick_pass.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create DAG parser helpers with test coverage</name>
  <files>
    rlm-service/processors/dag_parser.py
    rlm-service/processors/test_dag_parser.py
  </files>
  <action>
Create `rlm-service/processors/dag_parser.py` with three functions:

1. `extract_active_path(conversation: dict) -> list[dict]` — Backward traversal from `current_node` through `parent` chain.
   - Start at `conversation["current_node"]`, follow `mapping[node_id]["parent"]` backward to root
   - Collect each node's `message` object along the way
   - Reverse to chronological order
   - For each message, call `is_visible_message()` to filter, then `extract_content()` to get text
   - Return list of `{"role": str, "content": str, "create_time": float}` dicts (only messages with non-empty content)
   - FALLBACK: If `current_node` is missing or not in mapping, fall back to forward root traversal (find node with no parent, traverse first child at each level). Log warning: `"[dag_parser] WARNING: No current_node in conversation {id}, using fallback root traversal"`
   - If `mapping` key is missing entirely, check for `messages` key (pre-parsed format) and return those directly

2. `is_visible_message(message: dict) -> bool` — Filter hidden messages.
   - Return `False` if `author.role == "tool"` (DALL-E, browsing, code interpreter outputs)
   - Return `False` if `author.role == "system"` AND `metadata.is_user_system_message` is not True
   - Return `True` if `author.role in ["user", "assistant"]`
   - Return `False` for any other role (defensive default)

3. `extract_content(content_data: dict) -> str` — Extract all text from content.parts.
   - If `content_data` is None or empty, return `""`
   - If `content_data` has a `"text"` key directly, return that (some format variants)
   - Otherwise iterate `content_data.get("parts", [])`
   - For each part: if `isinstance(part, str)`, append it; if `isinstance(part, dict)` and has `"text"` key, append `part["text"]`; skip other dict parts (images with `asset_pointer`, etc.)
   - Join with `"\n"` and strip

Create `rlm-service/processors/test_dag_parser.py` with test cases:

Test `extract_active_path`:
- Test with branching conversation (2 children at one node, current_node points to one branch) — verify only active branch messages returned
- Test with linear conversation (no branches) — verify all messages in order
- Test with missing current_node — verify fallback produces messages (with warning log)
- Test with pre-parsed format (has `messages` key, no `mapping`) — verify passthrough

Test `is_visible_message`:
- Test user message returns True
- Test assistant message returns True
- Test tool message returns False
- Test system message returns False
- Test system message with is_user_system_message=True returns True
- Test unknown role returns False

Test `extract_content`:
- Test single string part: `{"parts": ["hello"]}` -> `"hello"`
- Test multiple string parts: `{"parts": ["hello", "world"]}` -> `"hello\nworld"`
- Test dict part with text: `{"parts": [{"text": "hello"}]}` -> `"hello"`
- Test mixed parts (string + dict + image): `{"parts": ["text", {"asset_pointer": "img"}, {"text": "more"}]}` -> `"text\nmore"`
- Test direct text field: `{"text": "hello"}` -> `"hello"`
- Test None input -> `""`
- Test empty parts: `{"parts": []}` -> `""`

Run tests with: `cd /home/drewpullen/clawd/soulprint-landing/rlm-service && python -m pytest processors/test_dag_parser.py -v` (install pytest if not present: `pip install pytest`)
  </action>
  <verify>
`cd /home/drewpullen/clawd/soulprint-landing/rlm-service && python -m pytest processors/test_dag_parser.py -v` — all tests pass
  </verify>
  <done>
dag_parser.py exports three functions. All test cases pass, covering branching conversations, message filtering, and polymorphic content extraction.
  </done>
</task>

<task type="auto">
  <name>Task 2: Integrate DAG parser into streaming import pipeline</name>
  <files>
    rlm-service/processors/streaming_import.py
  </files>
  <action>
Modify `parse_conversations_streaming()` in streaming_import.py to use DAG parser for proper message extraction.

Current behavior: Returns raw conversation objects (with `mapping` structure) directly. Downstream `sample_conversations()` looks for `.get('messages', [])` which is empty for ChatGPT export format.

New behavior: For each raw conversation from ijson, call `extract_active_path()` to get the active message list, then return conversations in the format that downstream expects.

Changes to `parse_conversations_streaming()`:
1. Add import at top of file: `from .dag_parser import extract_active_path`
2. After the ijson parsing loop (which produces `raw_convos` list from either format), add a processing step:

```python
# Process each conversation with DAG traversal
conversations = []
for raw_convo in raw_convos:
    parsed_messages = extract_active_path(raw_convo)

    if parsed_messages:
        conversations.append({
            "id": raw_convo.get("id"),
            "title": raw_convo.get("title", "Untitled"),
            "createdAt": datetime.fromtimestamp(
                raw_convo.get("create_time", 0)
            ).isoformat() if raw_convo.get("create_time") else datetime.now(timezone.utc).isoformat(),
            "messages": parsed_messages
        })

print(f"[streaming_import] Parsed {len(conversations)} conversations with DAG traversal ({sum(len(c['messages']) for c in conversations)} total messages)")
return conversations
```

3. Rename the intermediate variable from `conversations` to `raw_convos` in the ijson parsing block (lines 101-119) to distinguish raw from processed.

The key structural change: instead of `conversations = list(parser)` followed by `return conversations`, we now do `raw_convos = list(parser)` followed by DAG processing loop that produces properly structured conversations with `messages` key.

This ensures `sample_conversations()` in sample.py finds actual messages in `.get('messages', [])` instead of getting empty lists from raw `mapping`-based conversations.

Do NOT change the format detection logic (bare array vs wrapped) — it already works (PAR-04).
Do NOT change the download, progress tracking, or quick pass integration — only the parsing step.
  </action>
  <verify>
1. `python -c "from rlm-service.processors.streaming_import import parse_conversations_streaming"` — no import errors (run from project root, or verify with `cd /home/drewpullen/clawd/soulprint-landing/rlm-service && python -c 'from processors.streaming_import import parse_conversations_streaming'`)
2. Verify the function signature and return type match what `generate_quick_pass()` expects (list of dicts with `messages` key)
  </verify>
  <done>
`parse_conversations_streaming()` returns conversations with properly extracted `messages` arrays via DAG traversal. Each message has `role`, `content`, and `create_time` fields. Dead branches, tool messages, and hidden system messages are excluded. All content.parts are captured.
  </done>
</task>

<task type="auto">
  <name>Task 3: Update conversation chunker to use DAG parser</name>
  <files>
    rlm-service/processors/conversation_chunker.py
  </files>
  <action>
Update `format_conversation()` in conversation_chunker.py to use the shared DAG parser instead of its own forward root traversal.

Current behavior (lines 61-131): When conversation has `mapping` key, does forward traversal from root through `children` arrays with recursive `traverse_mapping()`. This captures dead branches and only skips `role == "system"` (misses `role == "tool"`). Content extraction uses `" ".join(str(part) for part in parts if part)` which stringifies dict parts.

New behavior: Use `extract_active_path()` from dag_parser.py for consistent DAG traversal.

Changes:
1. Add import at top: `from .dag_parser import extract_active_path`
2. Replace the entire `mapping`-based traversal section (the `else` branch starting at line 61 that handles `"mapping" in conversation`) with:

```python
    # Otherwise, handle ChatGPT export format with mapping
    # Use DAG traversal to extract only active conversation path
    parsed_messages = extract_active_path(conversation)

    for msg in parsed_messages:
        role = msg.get("role", "unknown")
        content = msg.get("content", "")

        # Truncate very long messages
        if len(content) > 5000:
            content = content[:5000] + "... [truncated]"

        if role == "user":
            formatted_lines.append(f"User: {content}")
        elif role == "assistant":
            formatted_lines.append(f"Assistant: {content}")

    return "\n".join(formatted_lines)
```

3. Remove the old `traverse_mapping()` inner function and the root-finding loop entirely. They are replaced by `extract_active_path()`.

4. Keep the `"messages" in conversation` branch (lines 38-59) as-is — that handles pre-parsed format and is still valid. Note: `extract_active_path()` also has this passthrough, but keeping the explicit branch in format_conversation is fine for clarity.

The `chunk_conversations()` function (lines 134-248) does NOT need changes — it calls `format_conversation()` which now uses DAG traversal internally.
  </action>
  <verify>
1. `cd /home/drewpullen/clawd/soulprint-landing/rlm-service && python -c "from processors.conversation_chunker import format_conversation, chunk_conversations; print('OK')"` — no import errors
2. Run the dag_parser tests again to confirm nothing broke: `cd /home/drewpullen/clawd/soulprint-landing/rlm-service && python -m pytest processors/test_dag_parser.py -v`
  </verify>
  <done>
conversation_chunker.py uses shared DAG parser for consistent traversal. Forward root traversal removed. Both streaming_import.py and conversation_chunker.py now use the same extract_active_path() function, ensuring consistent behavior across the entire import pipeline.
  </done>
</task>

<task type="auto">
  <name>Task 4: Sync to production RLM repo and push for Render deploy</name>
  <files>
    ~/clawd/soulprint-rlm/processors/dag_parser.py
    ~/clawd/soulprint-rlm/processors/streaming_import.py
    ~/clawd/soulprint-rlm/processors/conversation_chunker.py
  </files>
  <action>
The deployed RLM service runs from a SEPARATE repo at `~/clawd/soulprint-rlm/` (not from `soulprint-landing/rlm-service/`). Changes must be synced to this repo and pushed to trigger Render auto-deploy.

Copy the three modified/new files from the landing repo to the production RLM repo:

```bash
cp /home/drewpullen/clawd/soulprint-landing/rlm-service/processors/dag_parser.py /home/drewpullen/clawd/soulprint-rlm/processors/dag_parser.py
cp /home/drewpullen/clawd/soulprint-landing/rlm-service/processors/streaming_import.py /home/drewpullen/clawd/soulprint-rlm/processors/streaming_import.py
cp /home/drewpullen/clawd/soulprint-landing/rlm-service/processors/conversation_chunker.py /home/drewpullen/clawd/soulprint-rlm/processors/conversation_chunker.py
```

Then stage, commit, and push:

```bash
cd /home/drewpullen/clawd/soulprint-rlm
git add processors/dag_parser.py processors/streaming_import.py processors/conversation_chunker.py
git commit -m "feat: DAG traversal parsing — sync from soulprint-landing Phase 2"
git push origin main
```

This triggers Render auto-deploy. The deploy takes ~2-3 minutes.

Do NOT modify any other files in the RLM repo. Only sync the exact files changed in Tasks 1-3.
  </action>
  <verify>
1. `ls -la /home/drewpullen/clawd/soulprint-rlm/processors/dag_parser.py` — file exists
2. `diff /home/drewpullen/clawd/soulprint-landing/rlm-service/processors/dag_parser.py /home/drewpullen/clawd/soulprint-rlm/processors/dag_parser.py` — no differences
3. `diff /home/drewpullen/clawd/soulprint-landing/rlm-service/processors/streaming_import.py /home/drewpullen/clawd/soulprint-rlm/processors/streaming_import.py` — no differences
4. `diff /home/drewpullen/clawd/soulprint-landing/rlm-service/processors/conversation_chunker.py /home/drewpullen/clawd/soulprint-rlm/processors/conversation_chunker.py` — no differences
5. `cd /home/drewpullen/clawd/soulprint-rlm && git log --oneline -1` — shows the sync commit
  </verify>
  <done>
All modified processor files synced to production RLM repo and pushed to trigger Render auto-deploy. DAG traversal parsing is now live on the deployed service.
  </done>
</task>

</tasks>

<verification>
1. All DAG parser tests pass: `cd /home/drewpullen/clawd/soulprint-landing/rlm-service && python -m pytest processors/test_dag_parser.py -v`
2. No import errors in the module chain: `cd /home/drewpullen/clawd/soulprint-landing/rlm-service && python -c "from processors.streaming_import import parse_conversations_streaming; from processors.conversation_chunker import chunk_conversations; print('All imports OK')"`
3. Create an inline integration test with a realistic ChatGPT export structure (branching conversation with tool messages and multi-part content) and verify the pipeline produces clean output:
   - Only active path messages (no dead branches)
   - No tool or system messages
   - All text parts from multi-part messages
</verification>

<success_criteria>
- PAR-01: parse_conversations_streaming returns only active conversation path via current_node backward traversal
- PAR-02: Tool messages (role="tool") and system messages (role="system") are filtered out before soulprint generation
- PAR-03: All content.parts elements are extracted (strings and dict.text), not just parts[0]
- PAR-04: Both bare array and wrapped format still work (existing logic preserved)
- Fallback: Conversations without current_node still parse via root traversal with logged warning
- Consistency: Both streaming_import.py and conversation_chunker.py use the same dag_parser.py helpers
</success_criteria>

<output>
After completion, create `.planning/phases/02-parsing-quality/02-01-SUMMARY.md`
</output>
