---
phase: 02-full-pass-pipeline
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - supabase/migrations/20260207_full_pass_schema.sql
  - rlm-service/main.py
  - rlm-service/Dockerfile
  - rlm-service/requirements.txt
autonomous: true

must_haves:
  truths:
    - "POST /process-full endpoint accepts a job with user_id and storage_path and returns 202 immediately"
    - "Full pass processing runs in a background asyncio task, not blocking the HTTP response"
    - "Database has memory_md, full_pass_status, full_pass_started_at, full_pass_completed_at, full_pass_error columns"
    - "full_pass_status is set to 'processing' when job starts and 'failed' with error message on exception"
  artifacts:
    - path: "supabase/migrations/20260207_full_pass_schema.sql"
      provides: "Schema migration for memory_md and full_pass tracking columns"
      contains: "memory_md TEXT"
    - path: "rlm-service/main.py"
      provides: "/process-full endpoint with background task dispatch"
      contains: "process-full"
  key_links:
    - from: "rlm-service/main.py"
      to: "supabase user_profiles table"
      via: "httpx REST calls to update full_pass_status"
      pattern: "full_pass_status"
    - from: "app/api/import/process-server/route.ts"
      to: "rlm-service/main.py /process-full"
      via: "fire-and-forget POST from Phase 1 (already wired)"
      pattern: "process-full"
---

<objective>
Create the /process-full endpoint skeleton on the RLM service and the database migration for full pass tracking columns.

Purpose: The existing import pipeline (Phase 1) already fire-and-forgets to POST /process-full, but the endpoint doesn't exist yet. This plan creates the endpoint that accepts jobs and dispatches them as background asyncio tasks, plus the database columns needed to track progress and store the MEMORY section.

Output: Working /process-full endpoint that accepts jobs and runs a stub background task. SQL migration ready to execute. Dockerfile updated to support multi-file Python service.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-schema-quick-pass-pipeline/01-02-SUMMARY.md

Key existing code:
@rlm-service/main.py
@rlm-service/Dockerfile
@rlm-service/requirements.txt
@rlm-service/render.yaml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create database migration for full pass columns</name>
  <files>supabase/migrations/20260207_full_pass_schema.sql</files>
  <action>
Create SQL migration that adds the following columns to `public.user_profiles`:

```sql
ALTER TABLE public.user_profiles ADD COLUMN IF NOT EXISTS memory_md TEXT;
COMMENT ON COLUMN public.user_profiles.memory_md IS 'MEMORY section - curated durable facts (preferences, projects, dates, beliefs, decisions)';

ALTER TABLE public.user_profiles ADD COLUMN IF NOT EXISTS full_pass_status TEXT DEFAULT 'pending';
COMMENT ON COLUMN public.user_profiles.full_pass_status IS 'Background full pass status: pending, processing, complete, failed';

ALTER TABLE public.user_profiles ADD COLUMN IF NOT EXISTS full_pass_started_at TIMESTAMPTZ;
ALTER TABLE public.user_profiles ADD COLUMN IF NOT EXISTS full_pass_completed_at TIMESTAMPTZ;
ALTER TABLE public.user_profiles ADD COLUMN IF NOT EXISTS full_pass_error TEXT;

-- Check constraint for valid status values
ALTER TABLE public.user_profiles ADD CONSTRAINT full_pass_status_check
  CHECK (full_pass_status IN ('pending', 'processing', 'complete', 'failed'));
```

This is a user-executed migration (run in Supabase SQL Editor). Add a comment at the top noting this.

**IMPORTANT: This migration MUST be executed in Supabase SQL Editor BEFORE deploying the RLM service changes from Plans 02-02 and 02-03. The full pass pipeline writes to memory_md and full_pass_status columns â€” if they don't exist, the pipeline will fail silently (best-effort status updates).**
  </action>
  <verify>File exists at supabase/migrations/20260207_full_pass_schema.sql with all 5 columns and the check constraint.</verify>
  <done>Migration file ready to be executed in Supabase SQL Editor. Contains memory_md, full_pass_status, full_pass_started_at, full_pass_completed_at, full_pass_error columns with appropriate types and defaults.</done>
</task>

<task type="auto">
  <name>Task 2: Create /process-full endpoint with background task dispatch</name>
  <files>rlm-service/main.py, rlm-service/Dockerfile</files>
  <action>
Modify `rlm-service/main.py` to add:

1. **ProcessFullRequest Pydantic model:**
```python
class ProcessFullRequest(BaseModel):
    user_id: str
    storage_path: str
    conversation_count: int = 0
    message_count: int = 0
```

2. **POST /process-full endpoint:**
   - Validates the request
   - Returns 202 Accepted immediately with `{"status": "accepted", "message": "Full pass processing started"}`
   - Spawns `asyncio.create_task(run_full_pass(request))` as a background task BEFORE returning the response (use FastAPI's BackgroundTasks or `asyncio.create_task`)
   - Do NOT await the background task

3. **`run_full_pass(request: ProcessFullRequest)` async function:**
   - Wraps everything in try/except
   - On entry: Updates `full_pass_status = 'processing'` and `full_pass_started_at = now()` and `full_pass_error = None` via Supabase REST API (httpx POST)
   - Stub body: `await asyncio.sleep(1)` followed by a log message "Full pass stub complete for user {user_id}" -- Plan 02-02 will fill in the real logic
   - On success: Updates `full_pass_status = 'complete'` and `full_pass_completed_at = now()` (stub -- real completion in 02-02)
   - On failure: Updates `full_pass_status = 'failed'` and `full_pass_error = str(e)`, calls `await alert_failure(str(e), request.user_id, "Full pass failed")`

4. **Helper function `async def update_user_profile(user_id: str, updates: dict)`:**
   - Uses httpx to PATCH `{SUPABASE_URL}/rest/v1/user_profiles?user_id=eq.{user_id}` with the updates dict
   - Headers: `apikey: SUPABASE_SERVICE_KEY`, `Authorization: Bearer {SUPABASE_SERVICE_KEY}`, `Content-Type: application/json`, `Prefer: return=minimal`
   - Logs error if response is not 2xx but does not throw (best-effort status updates)

5. **Helper function `async def download_conversations(storage_path: str) -> list`:**
   - Downloads from Supabase Storage: GET `{SUPABASE_URL}/storage/v1/object/{storage_path}`
   - Headers: `Authorization: Bearer {SUPABASE_SERVICE_KEY}`
   - Decompresses if gzipped (check Content-Encoding or try gzip.decompress)
   - Parses JSON and returns the list of conversations
   - This is a real helper that Plan 02-02 will use

6. **Add `import asyncio` and `import gzip` to imports at top of file.**

7. **Update Dockerfile** to copy the entire rlm-service directory instead of just main.py:
   - Change `COPY main.py .` to `COPY . .` (since processors/ directory will be added in 02-02)
   - Keep the existing WORKDIR, EXPOSE, and CMD

IMPORTANT:
- Use `BackgroundTasks` from FastAPI for cleaner background task dispatch: `background_tasks: BackgroundTasks` parameter in the endpoint function, then `background_tasks.add_task(run_full_pass, request)`.
- The RLM service uses Anthropic SDK directly (NOT Bedrock). Model ID for Haiku 4.5 on Anthropic API is `claude-haiku-4-5-20251001`. No `us.anthropic.` prefix.
- Supabase REST API uses PATCH for updates, not POST. The URL pattern is `{SUPABASE_URL}/rest/v1/{table}?{column}=eq.{value}`.
  </action>
  <verify>
1. Read rlm-service/main.py and confirm:
   - ProcessFullRequest model exists
   - POST /process-full endpoint exists and returns 202
   - run_full_pass function exists with try/except and status updates
   - update_user_profile helper exists
   - download_conversations helper exists
2. Read rlm-service/Dockerfile and confirm COPY . . is used
3. Run `cd /home/drewpullen/clawd/soulprint-landing && python3 -c "import ast; ast.parse(open('rlm-service/main.py').read()); print('Python syntax OK')"` to validate syntax
  </verify>
  <done>
/process-full endpoint accepts jobs via POST, returns 202 immediately, and dispatches a background task that updates full_pass_status in the database. Dockerfile updated to support multi-file service. download_conversations helper ready for Plan 02-02.
  </done>
</task>

</tasks>

<verification>
1. SQL migration file contains all 5 columns with correct types
2. /process-full endpoint parses ProcessFullRequest and returns 202
3. Background task updates full_pass_status to 'processing' on start
4. Background task updates full_pass_status to 'failed' with error on exception
5. download_conversations helper correctly constructs Supabase Storage URL
6. Python syntax is valid (ast.parse succeeds)
7. Dockerfile copies entire directory, not just main.py
</verification>

<success_criteria>
- Migration SQL ready for execution (all 5 columns, check constraint)
- /process-full endpoint exists, returns 202, spawns background task
- Status tracking helpers (update_user_profile) work with Supabase REST API
- download_conversations helper implements storage download + gzip decompression
- Dockerfile supports multi-file Python service structure
</success_criteria>

<output>
After completion, create `.planning/phases/02-full-pass-pipeline/02-01-SUMMARY.md`
</output>
