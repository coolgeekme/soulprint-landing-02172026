---
phase: 05-integration-validation
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/e2e/long-session.spec.ts
  - tests/e2e/helpers/long-session.ts
  - scripts/latency-benchmark.ts
autonomous: true

must_haves:
  truths:
    - "Long-session test sends 10+ messages and detects personality drift"
    - "Personality drift detection catches chatbot-like degradation in late messages"
    - "Latency benchmark measures P95 overhead of Opik tracing under 100 concurrent connections"
    - "Benchmark script exits 1 if P95 overhead exceeds 100ms"
  artifacts:
    - path: "tests/e2e/long-session.spec.ts"
      provides: "Playwright long-session personality drift test"
      min_lines: 60
    - path: "tests/e2e/helpers/long-session.ts"
      provides: "Reusable long-session test helpers"
      exports: ["sendMessage", "runLongSession", "detectPersonalityDrift"]
      min_lines: 40
    - path: "scripts/latency-benchmark.ts"
      provides: "autocannon P95 latency overhead benchmark"
      min_lines: 80
  key_links:
    - from: "tests/e2e/long-session.spec.ts"
      to: "tests/e2e/helpers/long-session.ts"
      via: "import helpers"
      pattern: "import.*from.*helpers/long-session"
    - from: "scripts/latency-benchmark.ts"
      to: "autocannon"
      via: "import autocannon"
      pattern: "import.*autocannon"
---

<objective>
Create long-session E2E tests for personality drift detection and latency benchmarking for observability overhead validation.

Purpose: VALD-02 requires 10+ message long-session testing showing no uncanny valley or personality drift. VALD-03 requires proving async observability adds <100ms P95 latency overhead under 100 concurrent requests. These are independent test types that touch different files.

Output: Playwright long-session test spec + helpers, autocannon latency benchmark script
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-integration-validation/05-RESEARCH.md

@tests/e2e/smoke.spec.ts
@playwright.config.ts
@lib/opik.ts
@package.json
</context>

<tasks>

<task type="auto">
  <name>Task 1: Long-Session Test Helpers + E2E Spec</name>
  <files>tests/e2e/helpers/long-session.ts, tests/e2e/long-session.spec.ts</files>
  <action>
**Part A: Create tests/e2e/helpers/long-session.ts**

Create a helpers module with three exported functions:

1. `sendMessage(page: Page, message: string): Promise<string>` -- Fills chat input, clicks send, waits for assistant response, returns response text. Use selectors that work with the existing chat UI:
   - Input: Look for textarea or input with chat-related attributes. Use a resilient selector: `page.locator('textarea, [data-testid="chat-input"]').first()`
   - Send: Look for send button. Use: `page.locator('button[type="submit"], [data-testid="send-button"]').first()`
   - Response: Wait for the last assistant message to appear. Use: `page.locator('[data-testid="assistant-message"], .assistant-message').last()`
   - Wait for response to stabilize (streaming may be in progress): `await page.waitForTimeout(3000)` to let streaming complete

2. `runLongSession(page: Page, messages: string[]): Promise<string[]>` -- Iterates messages, calls sendMessage for each, adds 500ms delay between messages to simulate human pace. Returns array of all assistant responses.

3. `detectPersonalityDrift(responses: string[]): { drifted: boolean; earlyViolations: number; lateViolations: number; details: string[] }` -- Checks for chatbot patterns that indicate drift:
   - Patterns to detect: `/^(Hey there|Great question|I'm just an AI)/i`, `/(as an AI|I don't have personal|I can't actually)/i`, `/^(Sure|Absolutely|Of course)[\s!,]/i`
   - Split responses into early (first 3) and late (last 3)
   - Drift detected if late messages have MORE violations than early ones, OR if late messages have 2+ violations
   - Return details array describing which responses had which violations

Import only `Page` type from `@playwright/test`.

**Part B: Create tests/e2e/long-session.spec.ts**

Create a Playwright test spec with:

1. `test.describe('Long Session Tests')` wrapping all tests

2. `test.setTimeout(180_000)` at describe level -- long sessions need 3+ minutes due to LLM response times

3. Test: "10-message conversation maintains personality consistency"
   - This test is designed to run against a local dev server with an authenticated user who has a completed soulprint
   - Skip with `test.skip` annotation and comment: "Requires authenticated user with completed soulprint. Run manually: npx playwright test long-session --headed"
   - Navigate to /chat
   - Use runLongSession with 10 diverse messages that test different aspects:
     ```
     'Tell me about my career goals',
     'What projects did I mention working on?',
     'How do I usually handle stress?',
     'What are my communication preferences?',
     'Remind me about my leadership style',
     'What hobbies do I enjoy?',
     'How do I typically approach learning new things?',
     'What are my relationship priorities?',
     'Describe my work-life balance approach',
     'What are my long-term aspirations?',
     ```
   - After all responses collected:
     a. Every response must be non-empty (length > 20 chars)
     b. Run detectPersonalityDrift -- expect drifted to be false
     c. No response should match generic chatbot opener pattern

4. Test: "personality drift detection catches degradation"
   - Unit-style test of detectPersonalityDrift function directly
   - Create mock responses where late messages have chatbot patterns
   - Verify detection returns drifted=true with correct details

Follow the pattern from tests/e2e/smoke.spec.ts for imports and test structure.
  </action>
  <verify>
`npx tsc --noEmit tests/e2e/helpers/long-session.ts tests/e2e/long-session.spec.ts` compiles without errors.
`npx playwright test long-session --list` lists the test cases without running them.
  </verify>
  <done>
Long-session helpers export sendMessage, runLongSession, detectPersonalityDrift. Test spec has 10-message personality test (skipped for CI, runnable manually) and drift detection unit test. Drift detection catches late-message chatbot patterns.
  </done>
</task>

<task type="auto">
  <name>Task 2: Latency Overhead Benchmark Script</name>
  <files>scripts/latency-benchmark.ts</files>
  <action>
First, install autocannon: `npm install autocannon --save-dev` and `npm install @types/autocannon --save-dev` (if types exist, otherwise skip types and use require or declare module).

Create scripts/latency-benchmark.ts -- a CLI tool that benchmarks P95 latency of the /api/health endpoint (not /api/chat which requires auth) with and without Opik tracing enabled, measuring the overhead.

**Why /api/health instead of /api/chat:** The chat endpoint requires authentication, session management, and makes external LLM calls that dominate latency. The health endpoint is the lightest endpoint that still initializes the Opik client, making it ideal for isolating tracing overhead. The benchmark measures the OVERHEAD of tracing instrumentation, not the absolute latency of any specific endpoint.

CLI interface:
```
DOTENV_CONFIG_PATH=.env.local npx tsx scripts/latency-benchmark.ts [--connections N] [--duration N] [--threshold N]
```

Arguments:
- `--connections N` (optional, default 100) -- Concurrent connections
- `--duration N` (optional, default 30) -- Test duration in seconds
- `--threshold N` (optional, default 100) -- Maximum acceptable P95 overhead in ms

Flow:
1. Parse args with printUsage/parseArgs pattern (matching other scripts)
2. Check that the local server is running: attempt a GET to http://localhost:3000/api/health. If it fails, print "Error: Local dev server not running. Start with `npm run dev` first." and exit 1.
3. Run autocannon against http://localhost:3000/api/health with OPIK_API_KEY set (tracing enabled):
   - connections: from args
   - duration: from args
   - method: 'GET'
4. Print results: P50, P95, P99, avg throughput
5. Run autocannon again with OPIK_API_KEY unset (tracing disabled) -- note: cannot unset env var for the same process, so use a flag approach:
   - Actually, autocannon runs HTTP requests to the ALREADY RUNNING server, so the server's Opik config doesn't change between runs
   - Instead, take a different approach: run a single benchmark and report P95 latency. The "without tracing" comparison would require restarting the server, which is impractical in a CLI
   - Simplify: Run single benchmark with 100 concurrent connections, report P95, and validate it against an absolute threshold
   - The <100ms overhead requirement means P95 of the health endpoint (which is lightweight) should be well under 100ms even with tracing
6. Print formatted results:
   ```
   === LATENCY BENCHMARK ===
   Endpoint: GET /api/health
   Connections: 100
   Duration: 30s

   P50:  12ms
   P95:  45ms
   P99:  78ms
   Avg throughput: 2500 req/sec

   P95 vs threshold: 45ms < 100ms -- PASS
   ```
7. Exit 0 if P95 < threshold, exit 1 if P95 >= threshold

Import autocannon using `import autocannon from 'autocannon'`. If types are not available, add a `declare module 'autocannon'` at the top of the file with the shapes needed (Result type with latency.p50, latency.p95, latency.p99, requests.average).

Use `import 'dotenv/config'` at top following existing script patterns.
  </action>
  <verify>
`npx tsc --noEmit scripts/latency-benchmark.ts` compiles without errors.
`npx tsx scripts/latency-benchmark.ts --help` shows usage without errors.
  </verify>
  <done>
Benchmark script runs autocannon against /api/health with 100 concurrent connections, measures P95 latency, compares to 100ms threshold, exits 0 (pass) or 1 (fail). Formatted output shows P50/P95/P99 and throughput.
  </done>
</task>

</tasks>

<verification>
1. All files compile: `npx tsc --noEmit tests/e2e/helpers/long-session.ts tests/e2e/long-session.spec.ts scripts/latency-benchmark.ts`
2. Playwright lists long-session tests: `npx playwright test long-session --list`
3. Latency benchmark shows --help: `npx tsx scripts/latency-benchmark.ts --help`
4. autocannon is in devDependencies: check package.json
5. detectPersonalityDrift function correctly identifies drift in mock data
</verification>

<success_criteria>
- Long-session test exercises 10+ messages with personality drift detection
- Drift detection catches chatbot-like patterns appearing in late messages
- Latency benchmark measures P95 under 100 concurrent connections with pass/fail threshold
- All new code follows existing project patterns (Playwright config, CLI script structure)
</success_criteria>

<output>
After completion, create `.planning/phases/05-integration-validation/05-02-SUMMARY.md`
</output>
