---
phase: 05-integration-validation
plan: 03
type: execute
wave: 2
depends_on: ["05-01"]
files_modified:
  - .github/workflows/llm-regression.yml
  - scripts/quality-correlation.ts
autonomous: true

must_haves:
  truths:
    - "GitHub Actions workflow runs prompt regression tests on PRs that touch prompt files"
    - "Workflow uses regression-test.ts with minimum 20 samples and pass/fail exit codes"
    - "Quality correlation script computes Pearson r between quality scores and satisfaction proxies"
    - "Correlation script validates r > 0.7 threshold"
  artifacts:
    - path: ".github/workflows/llm-regression.yml"
      provides: "CI/CD automation for prompt regression testing"
      contains: "regression-test.ts"
      min_lines: 30
    - path: "scripts/quality-correlation.ts"
      provides: "Quality score vs satisfaction correlation validator"
      min_lines: 60
  key_links:
    - from: ".github/workflows/llm-regression.yml"
      to: "scripts/regression-test.ts"
      via: "npx tsx scripts/regression-test.ts"
      pattern: "regression-test"
    - from: "scripts/quality-correlation.ts"
      to: "lib/evaluation/quality-scoring.ts"
      via: "import quality scoring types"
      pattern: "import.*from.*quality"
---

<objective>
Create CI/CD automation for prompt regression testing and quality score correlation validation.

Purpose: VALD-01 requires regression tests to run BEFORE deploy (CI/CD integration). Success Criterion #5 requires quality scores to correlate r>0.7 with user satisfaction metrics. This plan wires the regression CLI (from Plan 01) into GitHub Actions and builds a correlation validation script.

Output: GitHub Actions workflow for automated regression testing, quality correlation validation script
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-integration-validation/05-RESEARCH.md

@scripts/regression-test.ts
@lib/evaluation/quality-scoring.ts
@lib/evaluation/quality-judges.ts
@package.json
</context>

<tasks>

<task type="auto">
  <name>Task 1: GitHub Actions LLM Regression Workflow</name>
  <files>.github/workflows/llm-regression.yml</files>
  <action>
Create .github/workflows/ directory if it does not exist, then create .github/workflows/llm-regression.yml.

This workflow runs prompt regression tests automatically on PRs that modify prompt-related files.

```yaml
name: LLM Regression Tests

on:
  pull_request:
    paths:
      - 'lib/soulprint/prompt-builder.ts'
      - 'lib/soulprint/prompt-helpers.ts'
      - 'lib/soulprint/emotional-intelligence.ts'
      - 'lib/evaluation/judges.ts'
      - 'lib/evaluation/quality-judges.ts'
      - 'lib/evaluation/quality-scoring.ts'
      - 'rlm-service/prompt_builder.py'

  # Allow manual triggering for ad-hoc regression checks
  workflow_dispatch:
    inputs:
      dataset:
        description: 'Opik dataset name'
        required: true
        type: string
      variant:
        description: 'Prompt variant (v1, v2-natural-voice)'
        required: true
        default: 'v2-natural-voice'
        type: choice
        options:
          - v1
          - v2-natural-voice
      samples:
        description: 'Number of samples (min 20)'
        required: false
        default: '20'
        type: string
```

Job configuration:
- `regression-test` job:
  - runs-on: ubuntu-latest
  - timeout-minutes: 15 (LLM evaluation can be slow)
  - Steps:
    1. actions/checkout@v4
    2. actions/setup-node@v4 with node-version '20' and cache 'npm'
    3. npm ci
    4. "Run prompt regression tests" step with env vars from secrets:
       - OPIK_API_KEY: ${{ secrets.OPIK_API_KEY }}
       - AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
       - AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
       - AWS_REGION: us-east-1
       - For pull_request trigger: use default dataset name 'chat-eval-regression', variant 'v2-natural-voice', samples 20
       - For workflow_dispatch trigger: use inputs
       - Run command: `npx tsx scripts/regression-test.ts --dataset $DATASET --variant $VARIANT --samples $SAMPLES`
       - Use shell env vars to handle both triggers:
         ```
         DATASET: ${{ github.event.inputs.dataset || 'chat-eval-regression' }}
         VARIANT: ${{ github.event.inputs.variant || 'v2-natural-voice' }}
         SAMPLES: ${{ github.event.inputs.samples || '20' }}
         ```
    5. "Upload results" step (if: always()) using actions/upload-artifact@v4 to save any results output

Add a comment at the top explaining:
- This workflow runs LLM-as-judge regression tests on prompt changes
- Requires OPIK_API_KEY, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY as repository secrets
- Creates Opik experiments tracked in the dashboard
- Exit code 0 = all metrics pass, 1 = regression detected
  </action>
  <verify>
Verify YAML syntax is valid: `python3 -c "import yaml; yaml.safe_load(open('.github/workflows/llm-regression.yml'))"` (or `npx yaml-lint .github/workflows/llm-regression.yml` if available). If neither tool available, manually verify the YAML structure is correct.
  </verify>
  <done>
GitHub Actions workflow file exists at .github/workflows/llm-regression.yml, triggers on PRs touching prompt files, runs regression-test.ts with proper env vars, supports manual dispatch.
  </done>
</task>

<task type="auto">
  <name>Task 2: Quality Score Correlation Validation Script</name>
  <files>scripts/quality-correlation.ts</files>
  <action>
Create scripts/quality-correlation.ts -- a CLI tool that validates quality scores correlate with user satisfaction proxies.

Follow the same CLI pattern as other scripts (dotenv/config, printUsage, parseArgs, main).

CLI interface:
```
DOTENV_CONFIG_PATH=.env.local npx tsx scripts/quality-correlation.ts [--min-profiles N] [--threshold N]
```

Arguments:
- `--min-profiles N` (optional, default 10) -- Minimum number of profiles needed for correlation to be meaningful
- `--threshold N` (optional, default 0.7) -- Minimum Pearson r correlation required to pass

Required env vars: NEXT_PUBLIC_SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY

Flow:
1. Parse args, validate env vars
2. Create Supabase admin client (import createClient from @supabase/supabase-js, use service role key)
3. Query user_profiles for all profiles where quality_breakdown IS NOT NULL
4. For each profile, also query chat_messages to compute satisfaction proxies:
   - `avg_session_length`: Average number of messages per conversation for this user
   - `total_conversations`: Number of conversations
   - `return_rate`: Number of distinct days the user chatted (a proxy for satisfaction)
5. Compute average quality score per profile from quality_breakdown JSONB:
   - Extract all section scores (soul, identity, user, agents, tools) x (completeness, coherence, specificity)
   - Average all 15 scores into a single composite quality score 0-100
6. Implement Pearson correlation coefficient computation:
   ```typescript
   function pearsonCorrelation(x: number[], y: number[]): number {
     const n = x.length;
     const sumX = x.reduce((a, b) => a + b, 0);
     const sumY = y.reduce((a, b) => a + b, 0);
     const sumXY = x.reduce((acc, xi, i) => acc + xi * y[i], 0);
     const sumX2 = x.reduce((acc, xi) => acc + xi * xi, 0);
     const sumY2 = y.reduce((acc, yi) => acc + yi * yi, 0);

     const numerator = n * sumXY - sumX * sumY;
     const denominator = Math.sqrt((n * sumX2 - sumX * sumX) * (n * sumY2 - sumY * sumY));

     if (denominator === 0) return 0;
     return numerator / denominator;
   }
   ```
7. Compute correlation between composite quality score and each satisfaction proxy:
   - quality vs avg_session_length
   - quality vs total_conversations
   - quality vs return_rate
8. Print results table:
   ```
   === QUALITY SCORE CORRELATION ===
   Profiles analyzed: 25

   Satisfaction Proxy        Pearson r    Status
   avg_session_length        0.73         PASS (>0.7)
   total_conversations       0.68         FAIL (<0.7)
   return_rate               0.81         PASS (>0.7)

   Best proxy: return_rate (r=0.81)
   Verdict: PASS (at least one proxy exceeds threshold)
   ```
9. PASS if at least ONE satisfaction proxy has r > threshold (since we're validating that quality scores ARE correlated with SOME satisfaction measure)
10. Exit 0 on pass, 1 on fail
11. If fewer profiles than --min-profiles, print warning "Insufficient data: {N} profiles found, need {min-profiles} for meaningful correlation" and exit 0 with INCONCLUSIVE status (not a failure, just insufficient data)

Import QualityBreakdown type from lib/evaluation/quality-scoring.ts for the JSONB shape.
  </action>
  <verify>
`npx tsc --noEmit scripts/quality-correlation.ts` compiles without errors.
`npx tsx scripts/quality-correlation.ts --help` shows usage without errors.
  </verify>
  <done>
Script queries profiles with quality scores, computes satisfaction proxies from chat_messages, calculates Pearson r correlation, validates at least one proxy exceeds r > 0.7, prints formatted results.
  </done>
</task>

</tasks>

<verification>
1. All files compile: `npx tsc --noEmit scripts/quality-correlation.ts`
2. GitHub Actions YAML is valid syntax
3. Workflow triggers on correct file paths (prompt-related files)
4. quality-correlation.ts shows usage with --help
5. Pearson correlation function produces correct output for known inputs (e.g., perfect positive correlation [1,2,3] vs [1,2,3] = 1.0)
</verification>

<success_criteria>
- GitHub Actions workflow runs regression-test.ts on PRs touching prompt files
- Workflow supports manual dispatch with configurable dataset/variant/samples
- Quality correlation script computes Pearson r between quality scores and multiple satisfaction proxies
- Correlation validation passes if at least one proxy exceeds r > 0.7 threshold
- Both artifacts follow existing project patterns
</success_criteria>

<output>
After completion, create `.planning/phases/05-integration-validation/05-03-SUMMARY.md`
</output>
