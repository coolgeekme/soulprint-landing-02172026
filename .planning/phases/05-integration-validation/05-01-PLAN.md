---
phase: 05-integration-validation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - scripts/regression-test.ts
  - scripts/baseline-compare.ts
  - lib/evaluation/statistical-validation.ts
autonomous: true

must_haves:
  truths:
    - "Developer can run regression tests against any prompt variant and get pass/fail result"
    - "Regression script enforces minimum 20 samples for statistical significance"
    - "Baseline comparison detects 5%+ degradation across personality_consistency, factuality, and tone_matching"
    - "CLI outputs clear pass/fail with per-metric scores and degradation percentages"
  artifacts:
    - path: "scripts/regression-test.ts"
      provides: "CLI prompt regression test runner"
      exports: ["main"]
      min_lines: 80
    - path: "scripts/baseline-compare.ts"
      provides: "CLI baseline comparison tool"
      exports: ["main"]
      min_lines: 60
    - path: "lib/evaluation/statistical-validation.ts"
      provides: "Sample size validation and baseline comparison utilities"
      exports: ["validateSampleSize", "compareToBaseline"]
      min_lines: 30
  key_links:
    - from: "scripts/regression-test.ts"
      to: "lib/evaluation/experiments.ts"
      via: "import runExperiment"
      pattern: "import.*runExperiment.*from.*experiments"
    - from: "scripts/regression-test.ts"
      to: "lib/evaluation/statistical-validation.ts"
      via: "import compareToBaseline"
      pattern: "import.*compareToBaseline.*from.*statistical-validation"
    - from: "scripts/baseline-compare.ts"
      to: "lib/evaluation/baseline.ts"
      via: "import recordBaseline"
      pattern: "import.*recordBaseline.*from.*baseline"
---

<objective>
Create the prompt regression testing CLI and baseline comparison infrastructure that catches personality degradation before deploy.

Purpose: VALD-01 requires a regression test suite with 20-100 cases that detects personality drift. This plan builds the CLI scripts and statistical validation library that underpin both local testing and CI/CD automation (Plan 03).

Output: Three artifacts -- regression-test.ts CLI, baseline-compare.ts CLI, and statistical-validation.ts library
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-integration-validation/05-RESEARCH.md

@lib/evaluation/experiments.ts
@lib/evaluation/baseline.ts
@lib/evaluation/types.ts
@lib/evaluation/judges.ts
@scripts/run-experiment.ts
@scripts/record-baseline.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Statistical Validation Library</name>
  <files>lib/evaluation/statistical-validation.ts</files>
  <action>
Create lib/evaluation/statistical-validation.ts with two exported functions:

1. `validateSampleSize(n: number): boolean` -- Returns true if n >= 20 (minimum for statistical power with medium effect size at 80% power, alpha 0.05).

2. `compareToBaseline(experimentScores: Record<string, MetricAggregate>, baselineScores: Record<string, MetricAggregate>, threshold?: number): ComparisonResult` where:
   - `threshold` defaults to 0.05 (5% degradation tolerance)
   - For each metric present in both experiment and baseline, compute degradation = (baseline.mean - experiment.mean) / baseline.mean
   - If degradation > threshold, add to degradations array
   - Return `{ passed: boolean; degradations: Array<{ metric: string; baseline: number; experiment: number; degradation: number }>; improvements: Array<{ metric: string; baseline: number; experiment: number; improvement: number }> }`

Import MetricAggregate from lib/evaluation/experiments.ts.

Also export `ComparisonResult` interface.

Follow existing patterns:
- Use the `import type` pattern for types (matching experiments.ts style)
- Add JSDoc comments matching the style in judges.ts
- Use Array.from() pattern for Map iteration (ES2017 target compatibility per STATE.md decisions)
  </action>
  <verify>
`npx tsc --noEmit lib/evaluation/statistical-validation.ts` compiles without errors.
Verify exports: validateSampleSize and compareToBaseline are exported.
  </verify>
  <done>
validateSampleSize(20) returns true, validateSampleSize(19) returns false. compareToBaseline detects degradation > 5% and returns passed=false with degradation details.
  </done>
</task>

<task type="auto">
  <name>Task 2: Prompt Regression Test CLI</name>
  <files>scripts/regression-test.ts</files>
  <action>
Create scripts/regression-test.ts -- a CLI tool that runs prompt regression tests against an Opik dataset and exits 0 (pass) or 1 (fail).

Follow the EXACT pattern from scripts/run-experiment.ts for:
- `import 'dotenv/config'` at top
- printUsage() function with clear documentation
- parseArgs() function returning typed args
- main() async function with error handling
- Required env var validation (OPIK_API_KEY, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)

CLI interface:
```
DOTENV_CONFIG_PATH=.env.local npx tsx scripts/regression-test.ts --dataset <name> --variant <name> [--samples N] [--threshold N]
```

Arguments:
- `--dataset <name>` (required) -- Opik dataset name
- `--variant <name>` (required) -- Prompt variant: v1, v2-natural-voice (use VARIANTS map from run-experiment.ts pattern)
- `--samples N` (optional, default 20) -- Number of samples (enforced minimum 20 via validateSampleSize)
- `--threshold N` (optional, default 0.05) -- Degradation threshold as decimal (0.05 = 5%)

Flow:
1. Parse args, validate env vars
2. Validate sample size >= 20 (exit 1 if not)
3. Run experiment using runExperiment() from lib/evaluation/experiments.ts with the selected variant
4. Define baseline thresholds as hardcoded defaults: personality_consistency: 0.70, factuality: 0.75, tone_matching: 0.70 (these are absolute minimums, not relative to a recorded baseline)
5. Compare each metric's mean to threshold -- if below, mark as failure
6. Print formatted table of results (metric name, score, threshold, pass/fail)
7. Print overall PASS/FAIL
8. Exit 0 on pass, 1 on fail

Import prompt variants from lib/evaluation/baseline.ts (v1PromptVariant, v2PromptVariant) and build VARIANTS map locally (same pattern as run-experiment.ts).
  </action>
  <verify>
`npx tsc --noEmit scripts/regression-test.ts` compiles without errors.
`npx tsx scripts/regression-test.ts --help` shows usage without errors.
  </verify>
  <done>
Script parses all arguments, validates sample size >= 20, runs experiment, compares to thresholds, and exits with correct code (0 pass, 1 fail). --help shows usage.
  </done>
</task>

<task type="auto">
  <name>Task 3: Baseline Comparison CLI</name>
  <files>scripts/baseline-compare.ts</files>
  <action>
Create scripts/baseline-compare.ts -- a CLI tool that runs BOTH v1 and v2 variants against the same dataset and compares their scores to detect regressions.

Follow the same CLI pattern as the other scripts.

CLI interface:
```
DOTENV_CONFIG_PATH=.env.local npx tsx scripts/baseline-compare.ts --dataset <name> [--samples N] [--threshold N]
```

Arguments:
- `--dataset <name>` (required) -- Opik dataset name
- `--samples N` (optional, default 20) -- Number of samples
- `--threshold N` (optional, default 0.05) -- Maximum allowed degradation from v1 to v2

Flow:
1. Parse args, validate env vars, validate sample size
2. Run experiment with v1PromptVariant (using recordBaseline from lib/evaluation/baseline.ts)
3. Run experiment with v2PromptVariant
4. Use compareToBaseline from lib/evaluation/statistical-validation.ts to compare v2 scores against v1 scores
5. Print side-by-side comparison table:
   ```
   Metric                    V1 Baseline   V2 Current   Delta     Status
   personality_consistency   0.82          0.85         +3.7%     PASS
   factuality                0.88          0.79         -10.2%    FAIL
   tone_matching             0.75          0.77         +2.7%     PASS
   ```
6. Print improvements and degradations separately
7. Print overall verdict: PASS (no degradations beyond threshold) or FAIL (with details)
8. Exit 0 on pass, 1 on fail

Import from:
- lib/evaluation/baseline.ts: recordBaseline, v1PromptVariant, v2PromptVariant
- lib/evaluation/experiments.ts: runExperiment
- lib/evaluation/statistical-validation.ts: compareToBaseline, validateSampleSize
  </action>
  <verify>
`npx tsc --noEmit scripts/baseline-compare.ts` compiles without errors.
`npx tsx scripts/baseline-compare.ts --help` shows usage without errors.
  </verify>
  <done>
Script runs both variants, compares scores with degradation threshold, prints formatted comparison table, and exits with correct code.
  </done>
</task>

</tasks>

<verification>
1. All three files compile: `npx tsc --noEmit lib/evaluation/statistical-validation.ts scripts/regression-test.ts scripts/baseline-compare.ts`
2. Both CLI scripts show usage with --help flag
3. statistical-validation.ts exports are importable from both scripts
4. Scripts follow the exact same patterns as existing scripts/run-experiment.ts and scripts/record-baseline.ts
</verification>

<success_criteria>
- scripts/regression-test.ts runs prompt regression with pass/fail thresholds and exits 0 or 1
- scripts/baseline-compare.ts compares v1 vs v2 with degradation detection
- lib/evaluation/statistical-validation.ts enforces minimum 20 samples and detects > 5% degradation
- All scripts follow existing CLI patterns (dotenv, printUsage, parseArgs, main)
</success_criteria>

<output>
After completion, create `.planning/phases/05-integration-validation/05-01-SUMMARY.md`
</output>
