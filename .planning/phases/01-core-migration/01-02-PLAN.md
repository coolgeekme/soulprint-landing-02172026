---
phase: 01-core-migration
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - rlm-service/processors/quick_pass.py
  - rlm-service/processors/sample.py
  - rlm-service/processors/__init__.py
autonomous: true

must_haves:
  truths:
    - "RLM service can sample richest conversations from ChatGPT export using same scoring algorithm as TypeScript version"
    - "RLM service can generate quick pass soulprint sections using Haiku 4.5 via Bedrock"
    - "Python quick pass logic matches TypeScript behavior (same prompts, same output structure)"
  artifacts:
    - path: "rlm-service/processors/sample.py"
      provides: "Conversation sampling with richness scoring"
      exports: ["sample_conversations", "format_conversations_for_prompt"]
      min_lines: 80
    - path: "rlm-service/processors/quick_pass.py"
      provides: "Quick pass generation via Bedrock Haiku 4.5"
      exports: ["generate_quick_pass"]
      min_lines: 100
  key_links:
    - from: "rlm-service/processors/quick_pass.py"
      to: "anthropic.AnthropicBedrock"
      via: "import and client instantiation"
      pattern: "from anthropic import.*Bedrock|AnthropicBedrock"
    - from: "rlm-service/processors/quick_pass.py"
      to: "rlm-service/processors/sample.py"
      via: "import sample_conversations"
      pattern: "from .sample import sample_conversations"
---

<objective>
Port TypeScript quick pass logic (sampling + generation) to Python for RLM service.

Purpose: Enable RLM service to generate structured personality sections directly, eliminating dependency on Vercel's TypeScript quick-pass.ts.
Output: Python modules (sample.py, quick_pass.py) with identical behavior to TypeScript originals.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-core-migration/01-RESEARCH.md
@lib/soulprint/sample.ts
@lib/soulprint/quick-pass.ts
@lib/soulprint/prompts.ts
</context>

<tasks>

<task type="auto">
  <name>Port conversation sampling logic to Python</name>
  <files>rlm-service/processors/sample.py</files>
  <action>
Create rlm-service/processors/sample.py with identical logic to lib/soulprint/sample.ts:

**Constants (match TypeScript exactly):**
```python
MIN_MESSAGES = 4
DEFAULT_TARGET_TOKENS = 50000
HARD_CAP = 50
MIN_SELECTED = 5
CHARS_PER_TOKEN = 4
MAX_MESSAGE_LENGTH = 2000
```

**Functions to port:**

1. `sample_conversations(conversations: List[Dict], target_tokens: int = DEFAULT_TARGET_TOKENS) -> List[Dict]`
   - Filter conversations with < 4 messages
   - Score using: `message_count * 10 + user_message_substance + balance_score + recency_bonus`
   - Sort by score descending
   - Select until token budget met (force MIN_SELECTED even if over budget)
   - Hard cap at 50 conversations

2. `format_conversations_for_prompt(conversations: List[Dict]) -> str`
   - Format as: `=== Conversation: "Title" (YYYY-MM-DD) ===\nUser: content\nAssistant: content`
   - Truncate messages > 2000 chars with "... [truncated]"
   - Return joined string with double newlines between conversations

**Key differences from TypeScript:**
- Use `from typing import List, Dict` for type hints
- Use `datetime.fromisoformat()` instead of `new Date()`
- Use `sorted()` instead of `.sort()`

**DO NOT add logging** - keep it simple, logging will be added by caller.
  </action>
  <verify>
Test that sample.py produces correct output when called:
```bash
cd /home/drewpullen/clawd/soulprint-landing/rlm-service
python3 -c "
from processors.sample import sample_conversations, format_conversations_for_prompt

# Test basic functionality
conversations = [
    {
        'id': '1',
        'title': 'Test Conversation',
        'messages': [
            {'role': 'user', 'content': 'Hello, how are you?'},
            {'role': 'assistant', 'content': 'I am doing well, thank you!'},
            {'role': 'user', 'content': 'What is the weather like?'},
            {'role': 'assistant', 'content': 'I cannot check the weather.'},
        ],
        'createdAt': '2024-01-01T00:00:00Z'
    }
]

sampled = sample_conversations(conversations)
formatted = format_conversations_for_prompt(sampled)

# Verify output
assert len(sampled) > 0, 'Sampling returned empty list'
assert 'Test Conversation' in formatted, 'Title not in formatted output'
assert 'Hello, how are you?' in formatted, 'User message not in output'
assert 'assistant' in formatted.lower(), 'No assistant label in output'

print('sample.py OK - produces expected output')
"
```
  </verify>
  <done>sample.py created with sample_conversations and format_conversations_for_prompt functions producing expected output format matching TypeScript behavior</done>
</task>

<task type="auto">
  <name>Port quick pass generation logic to Python</name>
  <files>rlm-service/processors/quick_pass.py</files>
  <action>
Create rlm-service/processors/quick_pass.py with Bedrock Haiku 4.5 integration:

**Imports:**
```python
import os
import json
from typing import Optional, Dict, List
from anthropic import AnthropicBedrock
from .sample import sample_conversations, format_conversations_for_prompt
```

**Load QUICK_PASS_SYSTEM_PROMPT from lib/soulprint/prompts.ts:**
Copy the exact system prompt from prompts.ts into a Python string constant. This is the multi-paragraph prompt starting with "You are analyzing a user's ChatGPT conversation history..."

**Function: `generate_quick_pass(conversations: List[Dict]) -> Optional[Dict]`**

Logic (matching quick-pass.ts):
1. Call `sampled = sample_conversations(conversations)`
2. Call `formatted_text = format_conversations_for_prompt(sampled)`
3. If formatted_text is empty, return None
4. Initialize Bedrock client:
   ```python
   client = AnthropicBedrock(
       aws_access_key=os.getenv('AWS_ACCESS_KEY_ID'),
       aws_secret_key=os.getenv('AWS_SECRET_ACCESS_KEY'),
       aws_region=os.getenv('AWS_REGION', 'us-east-1')
   )
   ```
5. Call Haiku 4.5:
   ```python
   message = client.messages.create(
       model="us.anthropic.claude-haiku-4-5-20251001-v1:0",
       max_tokens=8192,
       temperature=0.7,
       system=QUICK_PASS_SYSTEM_PROMPT,
       messages=[{"role": "user", "content": formatted_text}]
   )
   ```
6. Parse response as JSON: `result = json.loads(message.content[0].text)`
7. Return result dict with {soul, identity, user, agents, tools} sections
8. On any exception, return None (fail-safe like TypeScript version)

**Important:** Match TypeScript's permissive error handling - never throw, always return None on failure. Quick pass must not block imports.
  </action>
  <verify>
Check that quick_pass.py can be imported and has correct structure:
```bash
cd /home/drewpullen/clawd/soulprint-landing/rlm-service
python3 -c "
from processors.quick_pass import generate_quick_pass, QUICK_PASS_SYSTEM_PROMPT
assert callable(generate_quick_pass), 'generate_quick_pass not callable'
assert len(QUICK_PASS_SYSTEM_PROMPT) > 100, 'System prompt too short'
print('quick_pass.py structure OK')
"
```
  </verify>
  <done>quick_pass.py created with generate_quick_pass function, Bedrock integration, and exact system prompt from TypeScript version</done>
</task>

<task type="auto">
  <name>Update processors __init__.py to export new modules</name>
  <files>rlm-service/processors/__init__.py</files>
  <action>
Add exports for sample and quick_pass modules to rlm-service/processors/__init__.py:

```python
from .sample import sample_conversations, format_conversations_for_prompt
from .quick_pass import generate_quick_pass

__all__ = [
    'sample_conversations',
    'format_conversations_for_prompt',
    'generate_quick_pass',
    # ... existing exports ...
]
```

This allows importing with `from processors import generate_quick_pass` instead of `from processors.quick_pass import generate_quick_pass`.

If __init__.py doesn't exist, create it with just these exports. If it exists, append to existing __all__ list.
  </action>
  <verify>
Check that processors package exports new functions:
```bash
cd /home/drewpullen/clawd/soulprint-landing/rlm-service
python3 -c "
from processors import generate_quick_pass, sample_conversations, format_conversations_for_prompt
print('Exports OK')
"
```
  </verify>
  <done>processors/__init__.py exports sample and quick_pass functions, importable from processors package</done>
</task>

</tasks>

<verification>
1. sample.py exists with sample_conversations and format_conversations_for_prompt matching TypeScript
2. Scoring algorithm implemented as: message_count * 10 + substance + balance + recency
3. quick_pass.py exists with generate_quick_pass using Bedrock Haiku 4.5
4. processors/__init__.py exports all three functions
5. Functions are importable without errors
6. Prompts match TypeScript exactly
</verification>

<success_criteria>
- Python quick pass logic functionally identical to TypeScript version
- Scoring algorithm matches TypeScript exactly (verified via functional test)
- Bedrock Haiku 4.5 integration works with anthropic SDK
- All functions importable from processors package
- Fail-safe error handling (returns None on any failure)
- Ready for Wave 2 integration into /import-full endpoint
</success_criteria>

<output>
After completion, create `.planning/phases/01-core-migration/01-02-SUMMARY.md`
</output>
