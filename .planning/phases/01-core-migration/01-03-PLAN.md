---
phase: 01-core-migration
plan: 03
type: execute
wave: 2
depends_on: ["01-01", "01-02"]
files_modified:
  - rlm-service/main.py
  - rlm-service/processors/streaming_import.py
autonomous: true

must_haves:
  truths:
    - "RLM /import-full endpoint accepts user_id and storage_path, returns 202 Accepted immediately"
    - "RLM streams ChatGPT export from Supabase Storage with constant memory usage (no OOM on 300MB+ files)"
    - "RLM updates database progress_percent at each pipeline stage (download, parse, quick pass)"
    - "Import processing runs fire-and-forget (endpoint returns before processing completes)"
  artifacts:
    - path: "rlm-service/processors/streaming_import.py"
      provides: "Streaming download, parse, and quick pass pipeline"
      exports: ["process_import_streaming"]
      min_lines: 200
    - path: "rlm-service/main.py"
      provides: "/import-full POST endpoint"
      contains: "@app.post(\"/import-full\")"
  key_links:
    - from: "rlm-service/main.py"
      to: "rlm-service/processors/streaming_import.py"
      via: "asyncio.create_task(process_import_streaming(...))"
      pattern: "asyncio.create_task.*process_import_streaming"
    - from: "streaming_import.py"
      to: "httpx.AsyncClient"
      via: "streaming download from Supabase Storage"
      pattern: "httpx.AsyncClient.*stream"
    - from: "streaming_import.py"
      to: "ijson.items"
      via: "streaming JSON parsing"
      pattern: "ijson.items"
    - from: "streaming_import.py"
      to: "generate_quick_pass"
      via: "import from processors"
      pattern: "from .quick_pass import generate_quick_pass"
---

<objective>
Create RLM /import-full endpoint with streaming download, ijson parsing, and quick pass generation.

Purpose: Move all heavy import processing off Vercel (1GB RAM, 300s timeout) to RLM on Render with constant-memory streaming for any size export.
Output: Working /import-full endpoint that processes 300MB+ exports without OOM, updates progress throughout.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-core-migration/01-RESEARCH.md
@app/api/import/process-server/route.ts
@rlm-service/main.py
</context>

<tasks>

<task type="auto">
  <name>Create streaming import processor module</name>
  <files>rlm-service/processors/streaming_import.py</files>
  <action>
Create rlm-service/processors/streaming_import.py with complete streaming pipeline:

**Pipeline stages (matching 01-RESEARCH.md Pattern 1-5):**

1. **Update progress helper** (called throughout pipeline):
```python
async def update_progress(user_id: str, percent: int, stage: str):
    """Update user_profiles with progress_percent and import_stage"""
    async with httpx.AsyncClient() as client:
        await client.patch(
            f"{SUPABASE_URL}/rest/v1/user_profiles?user_id=eq.{user_id}",
            json={
                "progress_percent": percent,
                "import_stage": stage,
                "updated_at": datetime.utcnow().isoformat()
            },
            headers={
                "apikey": SUPABASE_SERVICE_KEY,
                "Authorization": f"Bearer {SUPABASE_SERVICE_KEY}",
                "Content-Type": "application/json"
            }
        )
```

2. **Streaming download from Supabase Storage** (Research Pattern 2):
```python
async def download_streaming(storage_path: str):
    """Stream download from Supabase Storage, yield chunks"""
    # storage_path format: "user-imports/user-123/raw-123.json"
    bucket, path = storage_path.split('/', 1)
    url = f"{SUPABASE_URL}/storage/v1/object/{bucket}/{path}"

    async with httpx.AsyncClient(timeout=300.0) as client:
        async with client.stream('GET', url, headers={
            'Authorization': f'Bearer {SUPABASE_SERVICE_KEY}'
        }) as response:
            response.raise_for_status()
            async for chunk in response.aiter_bytes():
                yield chunk
```

3. **Streaming JSON parsing** (Research Pattern 1):
```python
async def parse_conversations_streaming(byte_stream):
    """Parse ChatGPT conversations.json with ijson (constant memory)"""
    # ijson.items(stream, 'item') for bare array: [...]
    # ijson.items(stream, 'conversations.item') for wrapped: { conversations: [...] }

    # Try bare array first (most common)
    parser = ijson.items(byte_stream, 'item')

    conversations = []
    for conv in parser:
        # Each conv is a dict with 'mapping', 'title', 'create_time'
        conversations.append(conv)

    return conversations
```

4. **Main pipeline function**:
```python
async def process_import_streaming(user_id: str, storage_path: str):
    """
    Complete streaming import pipeline with constant memory.

    Stages:
    0-20%: Download from Supabase Storage
    20-50%: Parse conversations with ijson
    50-100%: Generate quick pass soulprint
    """
    try:
        # Stage 1: Download (0-20%)
        await update_progress(user_id, 0, "Downloading export")

        # Collect byte stream into chunks for ijson
        chunks = []
        async for chunk in download_streaming(storage_path):
            chunks.append(chunk)

        # Stage 2: Parse (20-50%)
        await update_progress(user_id, 20, "Parsing conversations")

        # ijson needs a file-like object - use BytesIO
        from io import BytesIO
        byte_stream = BytesIO(b''.join(chunks))
        conversations = await parse_conversations_streaming(byte_stream)

        # Stage 3: Quick Pass (50-100%)
        await update_progress(user_id, 50, "Generating soulprint")

        from .quick_pass import generate_quick_pass
        quick_pass_result = generate_quick_pass(conversations)  # synchronous

        if quick_pass_result:
            # Save to database (matching process-server.ts structure)
            # soul_md, identity_md, user_md, agents_md, tools_md as JSON strings
            soul_md = json.dumps(quick_pass_result.get('soul', {}))
            identity_md = json.dumps(quick_pass_result.get('identity', {}))
            user_md = json.dumps(quick_pass_result.get('user', {}))
            agents_md = json.dumps(quick_pass_result.get('agents', {}))
            tools_md = json.dumps(quick_pass_result.get('tools', {}))

            ai_name = quick_pass_result.get('identity', {}).get('ai_name', 'Nova')
            archetype = quick_pass_result.get('identity', {}).get('archetype', 'Analyzing...')

            # Update user_profiles with quick pass results
            async with httpx.AsyncClient() as client:
                await client.patch(
                    f"{SUPABASE_URL}/rest/v1/user_profiles?user_id=eq.{user_id}",
                    json={
                        "soul_md": soul_md,
                        "identity_md": identity_md,
                        "user_md": user_md,
                        "agents_md": agents_md,
                        "tools_md": tools_md,
                        "ai_name": ai_name,
                        "archetype": archetype,
                        "import_status": "quick_ready",
                        "import_error": None,
                        "progress_percent": 100,
                        "import_stage": "Complete",
                        "updated_at": datetime.utcnow().isoformat()
                    },
                    headers={
                        "apikey": SUPABASE_SERVICE_KEY,
                        "Authorization": f"Bearer {SUPABASE_SERVICE_KEY}",
                        "Content-Type": "application/json"
                    }
                )
        else:
            # Quick pass failed - still mark as complete but with placeholder
            await update_progress(user_id, 100, "Complete (basic profile)")
            async with httpx.AsyncClient() as client:
                await client.patch(
                    f"{SUPABASE_URL}/rest/v1/user_profiles?user_id=eq.{user_id}",
                    json={
                        "import_status": "quick_ready",
                        "import_error": "Quick pass failed, using placeholder",
                        "progress_percent": 100,
                        "import_stage": "Complete"
                    },
                    headers={
                        "apikey": SUPABASE_SERVICE_KEY,
                        "Authorization": f"Bearer {SUPABASE_SERVICE_KEY}",
                        "Content-Type": "application/json"
                    }
                )

    except Exception as e:
        # Update status to failed with error message
        error_msg = str(e)[:500]  # Limit error message length
        async with httpx.AsyncClient() as client:
            await client.patch(
                f"{SUPABASE_URL}/rest/v1/user_profiles?user_id=eq.{user_id}",
                json={
                    "import_status": "failed",
                    "import_error": error_msg,
                    "updated_at": datetime.utcnow().isoformat()
                },
                headers={
                    "apikey": SUPABASE_SERVICE_KEY,
                    "Authorization": f"Bearer {SUPABASE_SERVICE_KEY}",
                    "Content-Type": "application/json"
                }
            )
        raise  # Re-raise for logging
```

**Key implementation notes:**
- Use `async/await` throughout (httpx is async)
- Update progress at EVERY stage (0%, 20%, 50%, 100%)
- Return None on quick pass failure (don't block import)
- Limit error messages to 500 chars (database constraint)
- Use SUPABASE_URL and SUPABASE_SERVICE_KEY from env
  </action>
  <verify>
Check that streaming_import.py has all required functions:
```bash
cd /home/drewpullen/clawd/soulprint-landing/rlm-service
python3 -c "
from processors.streaming_import import process_import_streaming, update_progress
assert callable(process_import_streaming), 'process_import_streaming not callable'
assert callable(update_progress), 'update_progress not callable'
print('streaming_import.py structure OK')
"
```
  </verify>
  <done>streaming_import.py created with update_progress, download_streaming, parse_conversations_streaming, and process_import_streaming functions using ijson and httpx</done>
</task>

<task type="auto">
  <name>Add /import-full endpoint to RLM main.py</name>
  <files>rlm-service/main.py</files>
  <action>
Add /import-full POST endpoint to rlm-service/main.py using fire-and-forget pattern (Research Pattern 4):

**Request model:**
```python
class ImportFullRequest(BaseModel):
    user_id: str
    storage_path: str
    conversation_count: int = 0
    message_count: int = 0
```

**Endpoint implementation:**
```python
@app.post("/import-full")
async def import_full(request: ImportFullRequest):
    """
    Accept import job, return 202 immediately.
    Processing happens in background via asyncio.create_task.

    DO NOT use BackgroundTasks for long jobs (>60s) - fire asyncio task instead.
    """
    from processors.streaming_import import process_import_streaming

    # Fire-and-forget long-running job
    asyncio.create_task(process_import_streaming(
        user_id=request.user_id,
        storage_path=request.storage_path
    ))

    return {
        "status": "accepted",
        "message": "Import processing started"
    }, 202
```

**Important:**
- Add `import asyncio` at top of file if not present
- Add ImportFullRequest model near other request models
- Place endpoint near other POST endpoints
- DO NOT use `BackgroundTasks` parameter - use `asyncio.create_task()` for jobs >60s (per research)
- Return 202 Accepted immediately, don't await the task
  </action>
  <verify>
Check that /import-full endpoint exists in main.py:
```bash
grep -A 10 "@app.post(\"/import-full\")" /home/drewpullen/clawd/soulprint-landing/rlm-service/main.py
```
  </verify>
  <done>/import-full endpoint added to main.py, uses asyncio.create_task for fire-and-forget processing, returns 202 Accepted immediately</done>
</task>

</tasks>

<verification>
1. streaming_import.py exists with complete pipeline (download, parse, quick pass, progress updates)
2. main.py has /import-full endpoint that returns 202 and fires background task
3. Pipeline uses ijson for constant-memory parsing
4. Pipeline uses httpx for streaming download
5. Progress updates at 0%, 20%, 50%, 100% with stage messages
6. Error handling updates import_status to 'failed' with specific error message
</verification>

<success_criteria>
- /import-full endpoint accepts POST with {user_id, storage_path}, returns 202 immediately
- Background task streams download from Supabase Storage using httpx
- ijson parses conversations with constant memory (no json.loads())
- generate_quick_pass called to create soulprint sections
- Database updated with progress at each stage
- Any errors saved to import_error column with specific message
- Ready for Wave 3 Vercel integration
</success_criteria>

<output>
After completion, create `.planning/phases/01-core-migration/01-03-SUMMARY.md`
</output>
