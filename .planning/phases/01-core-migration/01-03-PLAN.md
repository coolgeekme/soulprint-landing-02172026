---
phase: 01-core-migration
plan: 03
type: execute
wave: 2
depends_on: ["01-01", "01-02"]
files_modified:
  - rlm-service/main.py
  - rlm-service/processors/streaming_import.py
autonomous: true

must_haves:
  truths:
    - "RLM /import-full endpoint accepts user_id and storage_path, returns 202 Accepted immediately"
    - "RLM streams ChatGPT export from Supabase Storage with constant memory usage (no OOM on 300MB+ files)"
    - "RLM updates database progress_percent at each pipeline stage (download, parse, quick pass)"
    - "Import processing runs fire-and-forget (endpoint returns before processing completes)"
  artifacts:
    - path: "rlm-service/processors/streaming_import.py"
      provides: "Streaming download, parse, and quick pass pipeline"
      exports: ["process_import_streaming"]
      min_lines: 200
    - path: "rlm-service/main.py"
      provides: "/import-full POST endpoint"
      contains: "@app.post(\"/import-full\")"
  key_links:
    - from: "rlm-service/main.py"
      to: "rlm-service/processors/streaming_import.py"
      via: "asyncio.create_task(process_import_streaming(...))"
      pattern: "asyncio.create_task.*process_import_streaming"
    - from: "streaming_import.py"
      to: "httpx.AsyncClient"
      via: "streaming download from Supabase Storage"
      pattern: "httpx.AsyncClient.*stream"
    - from: "streaming_import.py"
      to: "ijson.items"
      via: "streaming JSON parsing"
      pattern: "ijson.items"
    - from: "streaming_import.py"
      to: "generate_quick_pass"
      via: "import from processors"
      pattern: "from .quick_pass import generate_quick_pass"
---

<objective>
Create RLM /import-full endpoint with streaming download, ijson parsing, and quick pass generation.

Purpose: Move all heavy import processing off Vercel (1GB RAM, 300s timeout) to RLM on Render with constant-memory streaming for any size export.
Output: Working /import-full endpoint that processes 300MB+ exports without OOM, updates progress throughout.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-core-migration/01-RESEARCH.md
@app/api/import/process-server/route.ts
@rlm-service/main.py
</context>

<tasks>

<task type="auto">
  <name>Create streaming import processor module</name>
  <files>rlm-service/processors/streaming_import.py</files>
  <action>
Create rlm-service/processors/streaming_import.py with complete streaming pipeline using temporary file approach for true constant-memory processing:

**Pipeline stages (matching 01-RESEARCH.md Pattern 1-5):**

1. **Update progress helper** (called throughout pipeline):
```python
async def update_progress(user_id: str, percent: int, stage: str):
    """Update user_profiles with progress_percent and import_stage"""
    async with httpx.AsyncClient() as client:
        await client.patch(
            f"{SUPABASE_URL}/rest/v1/user_profiles?user_id=eq.{user_id}",
            json={
                "progress_percent": percent,
                "import_stage": stage,
                "updated_at": datetime.utcnow().isoformat()
            },
            headers={
                "apikey": SUPABASE_SERVICE_KEY,
                "Authorization": f"Bearer {SUPABASE_SERVICE_KEY}",
                "Content-Type": "application/json"
            }
        )
```

2. **Streaming download to temporary file** (TRUE constant memory - no accumulation):
```python
import tempfile
import os

async def download_streaming(storage_path: str, temp_file_path: str):
    """
    Stream download from Supabase Storage directly to temp file.
    Writes chunk-by-chunk, never accumulating in memory.
    """
    # storage_path format: "user-imports/user-123/raw-123.json"
    bucket, path = storage_path.split('/', 1)
    url = f"{SUPABASE_URL}/storage/v1/object/{bucket}/{path}"

    async with httpx.AsyncClient(timeout=300.0) as client:
        async with client.stream('GET', url, headers={
            'Authorization': f'Bearer {SUPABASE_SERVICE_KEY}'
        }) as response:
            response.raise_for_status()

            # Write chunks directly to disk (constant memory)
            with open(temp_file_path, 'wb') as f:
                async for chunk in response.aiter_bytes():
                    f.write(chunk)  # Immediately write to disk, don't accumulate
```

3. **Streaming JSON parsing from file** (Research Pattern 1):
```python
def parse_conversations_streaming(file_path: str):
    """
    Parse ChatGPT conversations.json with ijson from file (constant memory).
    ijson reads from file handle directly, never loading entire file into RAM.
    """
    conversations = []

    with open(file_path, 'rb') as f:
        # ijson.items(stream, 'item') for bare array: [...]
        # ijson.items(stream, 'conversations.item') for wrapped: { conversations: [...] }

        # Try bare array first (most common)
        parser = ijson.items(f, 'item')

        for conv in parser:
            # Each conv is a dict with 'mapping', 'title', 'create_time'
            conversations.append(conv)

    return conversations
```

4. **Main pipeline function**:
```python
async def process_import_streaming(user_id: str, storage_path: str):
    """
    Complete streaming import pipeline with TRUE constant memory.

    Uses temporary file approach:
    1. Stream httpx download to temp file (chunk-by-chunk, no accumulation)
    2. Pass temp file to ijson for parsing (file handle, no memory load)
    3. Clean up temp file after processing

    Stages:
    0-20%: Download from Supabase Storage
    20-50%: Parse conversations with ijson
    50-100%: Generate quick pass soulprint
    """
    temp_file_path = None

    try:
        # Create temporary file
        fd, temp_file_path = tempfile.mkstemp(suffix='.json', prefix='soulprint_import_')
        os.close(fd)  # Close file descriptor, we'll use path

        # Stage 1: Download to temp file (0-20%)
        await update_progress(user_id, 0, "Downloading export")
        await download_streaming(storage_path, temp_file_path)

        # Stage 2: Parse from temp file (20-50%)
        await update_progress(user_id, 20, "Parsing conversations")
        conversations = parse_conversations_streaming(temp_file_path)

        # Stage 3: Quick Pass (50-100%)
        await update_progress(user_id, 50, "Generating soulprint")

        from .quick_pass import generate_quick_pass
        quick_pass_result = generate_quick_pass(conversations)  # synchronous

        if quick_pass_result:
            # Save to database (matching process-server.ts structure)
            # soul_md, identity_md, user_md, agents_md, tools_md as JSON strings
            soul_md = json.dumps(quick_pass_result.get('soul', {}))
            identity_md = json.dumps(quick_pass_result.get('identity', {}))
            user_md = json.dumps(quick_pass_result.get('user', {}))
            agents_md = json.dumps(quick_pass_result.get('agents', {}))
            tools_md = json.dumps(quick_pass_result.get('tools', {}))

            ai_name = quick_pass_result.get('identity', {}).get('ai_name', 'Nova')
            archetype = quick_pass_result.get('identity', {}).get('archetype', 'Analyzing...')

            # Update user_profiles with quick pass results
            async with httpx.AsyncClient() as client:
                await client.patch(
                    f"{SUPABASE_URL}/rest/v1/user_profiles?user_id=eq.{user_id}",
                    json={
                        "soul_md": soul_md,
                        "identity_md": identity_md,
                        "user_md": user_md,
                        "agents_md": agents_md,
                        "tools_md": tools_md,
                        "ai_name": ai_name,
                        "archetype": archetype,
                        "import_status": "quick_ready",
                        "import_error": None,
                        "progress_percent": 100,
                        "import_stage": "Complete",
                        "updated_at": datetime.utcnow().isoformat()
                    },
                    headers={
                        "apikey": SUPABASE_SERVICE_KEY,
                        "Authorization": f"Bearer {SUPABASE_SERVICE_KEY}",
                        "Content-Type": "application/json"
                    }
                )
        else:
            # Quick pass failed - still mark as complete but with placeholder
            await update_progress(user_id, 100, "Complete (basic profile)")
            async with httpx.AsyncClient() as client:
                await client.patch(
                    f"{SUPABASE_URL}/rest/v1/user_profiles?user_id=eq.{user_id}",
                    json={
                        "import_status": "quick_ready",
                        "import_error": "Quick pass failed, using placeholder",
                        "progress_percent": 100,
                        "import_stage": "Complete"
                    },
                    headers={
                        "apikey": SUPABASE_SERVICE_KEY,
                        "Authorization": f"Bearer {SUPABASE_SERVICE_KEY}",
                        "Content-Type": "application/json"
                    }
                )

    except Exception as e:
        # Update status to failed with error message
        error_msg = str(e)[:500]  # Limit error message length
        async with httpx.AsyncClient() as client:
            await client.patch(
                f"{SUPABASE_URL}/rest/v1/user_profiles?user_id=eq.{user_id}",
                json={
                    "import_status": "failed",
                    "import_error": error_msg,
                    "updated_at": datetime.utcnow().isoformat()
                },
                headers={
                    "apikey": SUPABASE_SERVICE_KEY,
                    "Authorization": f"Bearer {SUPABASE_SERVICE_KEY}",
                    "Content-Type": "application/json"
                }
            )
        raise  # Re-raise for logging

    finally:
        # Clean up temp file
        if temp_file_path and os.path.exists(temp_file_path):
            try:
                os.unlink(temp_file_path)
            except Exception:
                pass  # Best effort cleanup
```

**Key implementation notes:**
- Use `tempfile.mkstemp()` to create temp file with unique name
- `download_streaming` writes chunks directly to disk (no accumulation in RAM)
- `parse_conversations_streaming` passes file handle to ijson (ijson streams from disk)
- Clean up temp file in `finally` block
- Update progress at EVERY stage (0%, 20%, 50%, 100%)
- Return None on quick pass failure (don't block import)
- Limit error messages to 500 chars (database constraint)
- Use SUPABASE_URL and SUPABASE_SERVICE_KEY from env
  </action>
  <verify>
Check that streaming_import.py has all required functions and uses temp file approach:
```bash
cd /home/drewpullen/clawd/soulprint-landing/rlm-service
python3 -c "
from processors.streaming_import import process_import_streaming, update_progress
assert callable(process_import_streaming), 'process_import_streaming not callable'
assert callable(update_progress), 'update_progress not callable'
print('streaming_import.py structure OK')
"

# Verify temp file approach (no chunk accumulation)
grep -E "tempfile|mkstemp" processors/streaming_import.py || echo "ERROR: Missing tempfile approach"
grep -E "chunks = \[\]|chunks.append" processors/streaming_import.py && echo "ERROR: Still accumulating chunks" || echo "No chunk accumulation found (correct)"
```
  </verify>
  <done>streaming_import.py created with update_progress, download_streaming (to temp file), parse_conversations_streaming (from temp file), and process_import_streaming using true constant-memory streaming via temporary file</done>
</task>

<task type="auto">
  <name>Add /import-full endpoint to RLM main.py</name>
  <files>rlm-service/main.py</files>
  <action>
Add /import-full POST endpoint to rlm-service/main.py using fire-and-forget pattern (Research Pattern 4):

**Request model:**
```python
class ImportFullRequest(BaseModel):
    user_id: str
    storage_path: str
    conversation_count: int = 0
    message_count: int = 0
```

**Endpoint implementation:**
```python
@app.post("/import-full")
async def import_full(request: ImportFullRequest):
    """
    Accept import job, return 202 immediately.
    Processing happens in background via asyncio.create_task.

    DO NOT use BackgroundTasks for long jobs (>60s) - fire asyncio task instead.
    """
    from processors.streaming_import import process_import_streaming

    # Fire-and-forget long-running job
    asyncio.create_task(process_import_streaming(
        user_id=request.user_id,
        storage_path=request.storage_path
    ))

    return {
        "status": "accepted",
        "message": "Import processing started"
    }, 202
```

**Important:**
- Add `import asyncio` at top of file if not present
- Add ImportFullRequest model near other request models
- Place endpoint near other POST endpoints
- DO NOT use `BackgroundTasks` parameter - use `asyncio.create_task()` for jobs >60s (per research)
- Return 202 Accepted immediately, don't await the task
  </action>
  <verify>
Check that /import-full endpoint exists in main.py:
```bash
grep -A 10 "@app.post(\"/import-full\")" /home/drewpullen/clawd/soulprint-landing/rlm-service/main.py
```
  </verify>
  <done>/import-full endpoint added to main.py, uses asyncio.create_task for fire-and-forget processing, returns 202 Accepted immediately</done>
</task>

</tasks>

<verification>
1. streaming_import.py exists with complete pipeline (download to temp file, parse from temp file, quick pass, progress updates)
2. main.py has /import-full endpoint that returns 202 and fires background task
3. Pipeline uses temp file for TRUE constant-memory processing (no chunk accumulation)
4. Pipeline uses ijson with file handle for streaming parse
5. Progress updates at 0%, 20%, 50%, 100% with stage messages
6. Error handling updates import_status to 'failed' with specific error message
7. Temp file cleaned up in finally block
</verification>

<success_criteria>
- /import-full endpoint accepts POST with {user_id, storage_path}, returns 202 immediately
- Background task streams download to temp file using httpx (constant memory, no accumulation)
- ijson parses from temp file with file handle (constant memory)
- Temp file cleaned up after processing
- generate_quick_pass called to create soulprint sections
- Database updated with progress at each stage
- Any errors saved to import_error column with specific message
- No OOM on 300MB+ files (true constant-memory streaming)
- Ready for Wave 3 Vercel integration
</success_criteria>

<output>
After completion, create `.planning/phases/01-core-migration/01-03-SUMMARY.md`
</output>
