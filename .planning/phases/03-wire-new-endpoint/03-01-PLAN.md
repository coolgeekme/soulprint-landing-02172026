---
phase: 03-wire-new-endpoint
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - /home/drewpullen/clawd/soulprint-rlm/main.py
autonomous: true

must_haves:
  truths:
    - "/process-full-v2 endpoint accepts POST requests and returns 200 with version=v2"
    - "Health check returns processors_available=true when all processor modules import correctly"
    - "Health check returns 503 when processor imports fail"
    - "All 3 existing @app.on_event startup handlers still execute (migrated to lifespan)"
    - "Server starts without errors when all processor modules are present"
  artifacts:
    - path: "/home/drewpullen/clawd/soulprint-rlm/main.py"
      provides: "lifespan context manager, /process-full-v2 endpoint, enhanced /health"
      contains: "async def lifespan"
  key_links:
    - from: "/process-full-v2 endpoint"
      to: "processors.full_pass.run_full_pass_pipeline"
      via: "run_full_pass_v2_background wrapper function"
      pattern: "from processors\\.full_pass import run_full_pass_pipeline"
    - from: "lifespan startup"
      to: "processors modules"
      via: "import validation in lifespan"
      pattern: "from processors\\."
    - from: "/health endpoint"
      to: "processors.full_pass"
      via: "import check in health handler"
      pattern: "processors_available"
---

<objective>
Add the /process-full-v2 endpoint and migrate main.py to use FastAPI lifespan pattern with processor import validation.

Purpose: This is the core wiring that connects the v1.2 processor modules (Phase 2) to a callable endpoint in the production RLM service. The lifespan migration ensures processor imports are validated at startup (fail fast), and the enhanced health check provides runtime validation for Render's auto-restart mechanism.

Output: Modified main.py with lifespan context manager, /process-full-v2 endpoint, and enhanced /health endpoint.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-wire-new-endpoint/03-RESEARCH.md
@.planning/phases/01-dependency-extraction/01-01-SUMMARY.md
@.planning/phases/02-copy-modify-processors/02-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Migrate on_event handlers to lifespan + add processor import validation + enhance /health</name>
  <files>/home/drewpullen/clawd/soulprint-rlm/main.py</files>
  <action>
Modify main.py to replace the 3 @app.on_event("startup") handlers with a single lifespan context manager. This is CRITICAL because FastAPI ignores all @app.on_event() decorators when lifespan is set.

**Step 1: Add lifespan context manager at the top of main.py (after imports, before app creation)**

Add `from contextlib import asynccontextmanager` to imports.

Create the lifespan function:
```python
@asynccontextmanager
async def lifespan(app: FastAPI):
    """Startup/shutdown lifecycle with processor validation."""
    # Validate all processor modules import correctly (fail fast)
    print("[Startup] Validating processor modules...")
    try:
        from processors.conversation_chunker import chunk_conversations
        from processors.fact_extractor import extract_facts_parallel, consolidate_facts, hierarchical_reduce
        from processors.memory_generator import generate_memory_section
        from processors.v2_regenerator import regenerate_sections_v2, sections_to_soulprint_text
        from processors.full_pass import run_full_pass_pipeline
        print("[Startup] All processor modules imported successfully")
    except ImportError as e:
        print(f"[FATAL] Processor import failed: {e}")
        raise  # Crash the app - Render will not route traffic

    # Migrate existing startup logic from @app.on_event("startup") handlers:

    # 1. From startup_event() at line ~171: Check for stuck jobs
    # NOTE: The startup_event calls resume_stuck_jobs() which uses module-level SUPABASE_URL/SUPABASE_SERVICE_KEY.
    # These are set at module level (line 179-180), so they are available.
    await asyncio.sleep(2)
    await resume_stuck_jobs()

    # 2. From startup() at line ~1710: Check RLM/Bedrock availability
    if not RLM_AVAILABLE:
        print(f"[RLM] Warning: RLM library not available: {RLM_IMPORT_ERROR}")
    else:
        print("[RLM] RLM library loaded successfully")

    if BEDROCK_AVAILABLE and AWS_ACCESS_KEY_ID:
        print("[RLM] AWS Bedrock configured - using Titan embeddings")
    elif COHERE_API_KEY:
        print("[RLM] Cohere API key configured - using Cohere embeddings")
    else:
        print("[RLM] Warning: No embedding service configured")

    # 3. From startup_check_incomplete_embeddings() at line ~3555: Resume incomplete embeddings
    await asyncio.sleep(5)
    await startup_check_incomplete_embeddings_logic()

    yield  # Application runs here

    # Shutdown
    print("[Shutdown] Cleanup complete")
```

**Step 2: Extract the body of startup_check_incomplete_embeddings into a plain async function**

The current `startup_check_incomplete_embeddings()` at line ~3555 is decorated with `@app.on_event("startup")`. Extract its body into a new function `startup_check_incomplete_embeddings_logic()` (without the decorator). Place it right before the lifespan function. Keep the original logic intact — just remove the decorator and the `await asyncio.sleep(5)` (that delay is now in the lifespan).

**Step 3: Remove ALL 3 @app.on_event("startup") decorators**

Remove the decorators (and the original function definitions if their logic is now in lifespan):
1. Line ~171: `@app.on_event("startup") async def startup_event()` — Remove entirely (logic now in lifespan)
2. Line ~1710: `@app.on_event("startup") async def startup()` — Remove entirely (logic now in lifespan)
3. Line ~3555: `@app.on_event("startup") async def startup_check_incomplete_embeddings()` — Replace with plain function `startup_check_incomplete_embeddings_logic()` (no decorator)

**Step 4: Update app creation to use lifespan**

Change `app = FastAPI(title="SoulPrint RLM Service")` to `app = FastAPI(title="SoulPrint RLM Service", lifespan=lifespan)`

**Step 5: Enhance /health endpoint**

Replace the existing /health endpoint (line ~1726) with an enhanced version that validates processor imports:

```python
@app.get("/health")
async def health():
    """Health check with processor validation for Render auto-restart."""
    health_status = {
        "status": "ok",
        "service": "soulprint-rlm",
        "rlm_available": RLM_AVAILABLE,
        "bedrock_available": BEDROCK_AVAILABLE and bool(AWS_ACCESS_KEY_ID),
        "timestamp": datetime.utcnow().isoformat(),
    }

    # Validate processor imports (lightweight check)
    try:
        from processors.full_pass import run_full_pass_pipeline
        health_status["processors_available"] = True
    except ImportError as e:
        health_status["processors_available"] = False
        health_status["processor_error"] = str(e)
        # Return 503 to trigger Render auto-restart
        raise HTTPException(status_code=503, detail=f"Processor modules unavailable: {e}")

    return health_status
```

**IMPORTANT:** Preserve the existing rlm_available and bedrock_available fields in the health response — these are likely used by monitoring. Only ADD processors_available, don't remove existing fields.

**What NOT to change:**
- Do NOT modify any existing endpoint implementations (/process-full, /chat, /query, etc.)
- Do NOT modify the ProcessFullRequest Pydantic model
- Do NOT modify the process_full_background function
- Do NOT modify create_job, update_job, complete_job, get_stuck_jobs, resume_stuck_jobs functions
- Do NOT change module-level variable assignments (SUPABASE_URL, etc.)
- Do NOT modify /health-deep endpoint

**CAUTION about variable references in lifespan:** The lifespan function runs at module level after imports. Module-level variables like RLM_AVAILABLE, BEDROCK_AVAILABLE, AWS_ACCESS_KEY_ID, COHERE_API_KEY are defined AFTER the app creation in the current code. The lifespan runs at startup time (not import time), so these WILL be available. But verify the ordering: lifespan function is defined, app is created with lifespan param, module-level vars are defined, then at runtime the lifespan executes and reads those vars. This should work fine.
  </action>
  <verify>
Run from the soulprint-rlm directory:
```bash
cd /home/drewpullen/clawd/soulprint-rlm
python -c "from main import app; print('App created OK')"
```
Also verify no @app.on_event remains:
```bash
grep -n "on_event" /home/drewpullen/clawd/soulprint-rlm/main.py
```
Should return 0 results.

Verify lifespan is registered:
```bash
grep -n "lifespan" /home/drewpullen/clawd/soulprint-rlm/main.py
```
Should show the lifespan function definition and FastAPI(lifespan=lifespan).
  </verify>
  <done>
- main.py uses lifespan context manager (no @app.on_event decorators remain)
- Lifespan validates processor imports at startup (crashes on ImportError)
- Lifespan runs all 3 original startup tasks (stuck jobs, availability check, incomplete embeddings)
- /health endpoint returns processors_available field and 503 on import failure
- All existing endpoint definitions unchanged
  </done>
</task>

<task type="auto">
  <name>Task 2: Add /process-full-v2 endpoint with background task dispatch</name>
  <files>/home/drewpullen/clawd/soulprint-rlm/main.py</files>
  <action>
Add the /process-full-v2 endpoint to main.py. Place it immediately after the existing /process-full endpoint (after line ~2440).

**Step 1: Add the background task wrapper function**

Place this BEFORE the endpoint definition:

```python
async def run_full_pass_v2_background(
    user_id: str,
    storage_path: str,
    job_id: Optional[str] = None,
):
    """Background task wrapper for v2 pipeline (processors from Phase 2)."""
    try:
        from processors.full_pass import run_full_pass_pipeline

        print(f"[v2] Starting pipeline for user {user_id}")
        print(f"[v2] Storage path: {storage_path}")

        memory_md = await run_full_pass_pipeline(
            user_id=user_id,
            storage_path=storage_path,
        )

        print(f"[v2] Pipeline complete for user {user_id}")

        if job_id:
            await complete_job(job_id, success=True)

    except Exception as e:
        print(f"[v2] Pipeline FAILED for user {user_id}: {e}")
        import traceback
        traceback.print_exc()

        if job_id:
            await complete_job(job_id, success=False, error_message=str(e)[:500])
```

Note: Uses the EXISTING complete_job() function from main.py (not update_job). This is consistent with how v1 reports completion.

**Step 2: Add the /process-full-v2 endpoint**

```python
@app.post("/process-full-v2")
async def process_full_v2(request: ProcessFullRequest, background_tasks: BackgroundTasks):
    """
    V2 full processing pipeline using modular processors from v1.2.

    Pipeline: chunk conversations -> extract facts (parallel) -> consolidate ->
    generate MEMORY section -> regenerate v2 sections (SOUL, IDENTITY, USER, AGENTS, TOOLS)

    Uses processors/ modules (Phase 2) instead of inline main.py logic.
    Runs alongside v1 /process-full for gradual migration.
    """
    print(f"[v2] Received process-full-v2 request for user {request.user_id}")

    if not request.storage_path:
        raise HTTPException(
            status_code=400,
            detail="storage_path required for v2 pipeline (direct conversations not supported)"
        )

    # Create job record for recovery (reuses existing job system)
    job_id = await create_job(
        request.user_id,
        request.storage_path,
        request.conversation_count or 0,
        request.message_count or 0,
    )

    if job_id:
        print(f"[v2] Created job {job_id}")

    # Dispatch to background
    background_tasks.add_task(
        run_full_pass_v2_background,
        request.user_id,
        request.storage_path,
        job_id,
    )

    return {
        "status": "processing",
        "version": "v2",
        "message": "v2 pipeline started: chunk → facts → MEMORY → v2 sections",
        "user_id": request.user_id,
        "conversation_count": request.conversation_count,
        "job_id": job_id,
    }
```

**Key design decisions:**
- Reuses ProcessFullRequest model (shared with v1) — DO NOT create a new model
- Reuses create_job/complete_job (shared job tracking) — DO NOT create new job functions
- Requires storage_path (no legacy direct conversations support for v2)
- Returns version="v2" in response (distinguishes from v1)
- Uses BackgroundTasks (FastAPI built-in) for dispatch, NOT asyncio.create_task
- Uses complete_job() for final status (same as v1 pattern)
- Logs with [v2] prefix to distinguish from v1 [RLM] logs

**What NOT to change:**
- Do NOT modify the existing /process-full endpoint or process_full_background function
- Do NOT modify ProcessFullRequest model
- Do NOT modify create_job, update_job, complete_job functions
  </action>
  <verify>
Run from soulprint-rlm directory:
```bash
cd /home/drewpullen/clawd/soulprint-rlm
python -c "from main import app; routes = [r.path for r in app.routes]; assert '/process-full-v2' in routes; print('v2 endpoint registered')"
```
Also verify the existing v1 endpoint still exists:
```bash
python -c "from main import app; routes = [r.path for r in app.routes]; assert '/process-full' in routes; print('v1 endpoint still exists')"
```
  </verify>
  <done>
- /process-full-v2 endpoint is registered and accepts POST requests
- Endpoint dispatches run_full_pass_v2_background as BackgroundTask
- Endpoint requires storage_path (returns 400 without it)
- Endpoint returns version="v2" in response body
- Existing /process-full endpoint unchanged
- Job tracking reuses existing create_job/complete_job system
  </done>
</task>

</tasks>

<verification>
After both tasks complete:
1. `python -c "from main import app; print('OK')"` — No import errors
2. `grep -c "on_event" main.py` — Returns 0 (all migrated to lifespan)
3. `grep "lifespan" main.py` — Shows lifespan function and FastAPI(lifespan=...)
4. `grep "process-full-v2" main.py` — Shows new endpoint
5. `grep "processors_available" main.py` — Shows enhanced health check
6. All existing endpoint definitions still present (grep for each endpoint path)
</verification>

<success_criteria>
- main.py compiles without errors
- /process-full-v2 endpoint exists alongside /process-full
- Lifespan context manager validates processor imports at startup
- /health endpoint returns processors_available field
- All @app.on_event decorators removed (migrated to lifespan)
- No existing endpoint implementations modified
</success_criteria>

<output>
After completion, create `.planning/phases/03-wire-new-endpoint/03-01-SUMMARY.md`
</output>
