---
phase: 04-cost-quality-measurement
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - scripts/eval-memory-quality.ts
  - lib/evaluation/memory-quality-judge.ts
autonomous: true

must_haves:
  truths:
    - "A/B experiment script compares chat quality with full pass complete vs quick_ready only"
    - "Memory quality judge evaluates whether responses use deep memory (facts, history) vs generic personality"
    - "Experiment produces aggregate scores showing quality difference between conditions"
  artifacts:
    - path: "scripts/eval-memory-quality.ts"
      provides: "CLI script that runs A/B experiment comparing full_pass vs quick_ready prompt conditions"
      contains: "runExperiment"
    - path: "lib/evaluation/memory-quality-judge.ts"
      provides: "MemoryDepthJudge class that scores how well responses leverage deep memory"
      contains: "class MemoryDepthJudge"
  key_links:
    - from: "scripts/eval-memory-quality.ts"
      to: "lib/evaluation/experiments.ts"
      via: "import { runExperiment } from experiments"
      pattern: "runExperiment"
    - from: "scripts/eval-memory-quality.ts"
      to: "lib/evaluation/datasets.ts"
      via: "import { createEvaluationDataset } from datasets"
      pattern: "createEvaluationDataset"
    - from: "lib/evaluation/memory-quality-judge.ts"
      to: "lib/bedrock.ts"
      via: "bedrockChatJSON for LLM-as-judge scoring"
      pattern: "bedrockChatJSON"
---

<objective>
Create A/B evaluation experiment that measures chat quality improvement when full pass (deep memory) is complete vs quick_ready only.

Purpose: Requirement MEM-03 needs measurable evidence that full pass improves chat quality. This plan creates a MemoryDepthJudge that specifically scores whether responses leverage deep memory (facts, history, specific details) vs generic personality, and a CLI script that runs both conditions against the same dataset items using the existing Opik experiment infrastructure.

Output: MemoryDepthJudge class, A/B experiment CLI script
</objective>

<execution_context>
@/home/drewpullen/.claude/get-shit-done/workflows/execute-plan.md
@/home/drewpullen/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@lib/evaluation/experiments.ts
@lib/evaluation/judges.ts
@lib/evaluation/datasets.ts
@lib/evaluation/types.ts
@lib/soulprint/prompt-builder.ts
@scripts/create-eval-dataset.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create MemoryDepthJudge and A/B experiment script</name>
  <files>
    lib/evaluation/memory-quality-judge.ts
    scripts/eval-memory-quality.ts
  </files>
  <action>
    **Part A: Create `lib/evaluation/memory-quality-judge.ts`**

    Create a new LLM-as-judge metric following the pattern in `lib/evaluation/judges.ts`:

    ```typescript
    import { BaseMetric, z } from 'opik';
    import type { EvaluationScoreResult } from 'opik';
    import { bedrockChatJSON } from '@/lib/bedrock';
    ```

    Define `MemoryDepthJudge` extending `BaseMetric`:

    Schema (z.object):
    - `input`: z.string() (user message)
    - `output`: z.string() (assistant response)
    - `has_memory`: z.boolean() (whether full pass memory was available)
    - `memory_context`: z.string().optional() (the MEMORY section, if available)
    - `soulprint_context`: z.record(z.string(), z.unknown()).optional()

    The judge evaluates on a 0.0-1.0 scale how well the response demonstrates deep knowledge of the user:

    Scoring rubric:
    - 0.8-1.0: Response references specific user facts, projects, preferences, or history. Feels like talking to someone who deeply knows the user.
    - 0.6-0.79: Response shows some personalization beyond basic personality matching. References user context without being specific.
    - 0.4-0.59: Response matches personality tone but uses no specific user knowledge. Could apply to anyone with similar personality traits.
    - 0.0-0.39: Generic response with no evidence of personalization or memory usage.

    Include the ANTI_LENGTH_BIAS instruction (copy from judges.ts).
    Use `bedrockChatJSON` with `model: 'HAIKU_45'`, `maxTokens: 512`, `temperature: 0.1`.

    **Part B: Create `scripts/eval-memory-quality.ts`**

    A CLI script that runs two experiment conditions against the same dataset:

    1. **Condition A: quick_ready only** -- Build system prompt using PromptBuilder WITHOUT memory_md and WITHOUT memoryContext (simulates what users get before full pass completes)
    2. **Condition B: full_pass complete** -- Build system prompt using PromptBuilder WITH memory_md and WITH memoryContext from conversation_chunks (simulates what users get after full pass)

    Implementation:

    ```typescript
    import 'dotenv/config';
    import { createClient } from '@supabase/supabase-js';
    import { evaluate } from 'opik';
    import type { EvaluationResult } from 'opik';
    import { getOpikClient } from '@/lib/opik';
    import { bedrockChat } from '@/lib/bedrock';
    import { PromptBuilder } from '@/lib/soulprint/prompt-builder';
    import {
      PersonalityConsistencyJudge,
      FactualityJudge,
      ToneMatchingJudge,
    } from '@/lib/evaluation/judges';
    import { MemoryDepthJudge } from '@/lib/evaluation/memory-quality-judge';
    import type { ChatEvalItem } from '@/lib/evaluation/types';
    ```

    The script should:

    1. Parse CLI args: `--dataset <name>` (required), `--limit <n>` (optional, default 20), `--help`
    2. Validate env vars: NEXT_PUBLIC_SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY, OPIK_API_KEY, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY
    3. Fetch memory data for users in the dataset:
       - Get unique user_id_hashes from dataset items
       - For each user with soulprint context, fetch their memory_md and a sample of conversation_chunks from Supabase
       - Build a memory_context string from chunks (top 5 chunks concatenated)
    4. Define two PromptVariant-style configs:
       - `quick_ready`: Uses PromptBuilder v2-natural-voice with profile sections but NO memory_md, NO memoryContext
       - `full_pass`: Uses PromptBuilder v2-natural-voice with profile sections PLUS memory_md AND memoryContext (from fetched chunks)
    5. For each condition, run an Opik experiment with 4 judges:
       - PersonalityConsistencyJudge
       - FactualityJudge
       - ToneMatchingJudge
       - MemoryDepthJudge (with has_memory flag set appropriately)
    6. Print results comparison table:
       ```
       Metric                    | quick_ready | full_pass | Delta
       ========================= | =========== | ========= | =====
       personality_consistency   |    0.82     |   0.85    | +0.03
       factuality                |    0.78     |   0.84    | +0.06
       tone_matching             |    0.80     |   0.82    | +0.02
       memory_depth              |    0.35     |   0.72    | +0.37
       ```

    IMPORTANT: The script works with the EXISTING evaluation dataset format (ChatEvalItem). It fetches memory data separately using the user_id_hash from metadata to look up real user profiles. If a user has no memory_md yet, skip them from the experiment (log a warning).

    The script must handle the case where no users have full pass complete -- print a message saying "No users with full pass data found. Run a full pass import first, then re-run this experiment."

    Usage: `DOTENV_CONFIG_PATH=.env.local npx tsx scripts/eval-memory-quality.ts --dataset chat-eval-2026-02-11-1430 --limit 20`
  </action>
  <verify>
    1. `npm run build` succeeds
    2. grep confirms `class MemoryDepthJudge` in memory-quality-judge.ts
    3. grep confirms `MemoryDepthJudge` imported in eval-memory-quality.ts
    4. grep confirms `quick_ready` and `full_pass` experiment names in eval-memory-quality.ts
    5. grep confirms `PromptBuilder` used in eval-memory-quality.ts
    6. grep confirms `--dataset` arg parsing in eval-memory-quality.ts
    7. grep confirms results table output (personality_consistency, memory_depth) in eval-memory-quality.ts
  </verify>
  <done>
    MemoryDepthJudge scores 0.0-1.0 on how well responses leverage deep user memory vs generic personality. A/B experiment script runs two conditions (quick_ready without memory, full_pass with memory) against the same Opik dataset, scoring with 4 judges (3 existing + MemoryDepthJudge). Results printed as comparison table showing per-metric deltas. Script uses existing evaluation infrastructure (Opik, PromptBuilder, bedrockChat).
  </done>
</task>

</tasks>

<verification>
1. Next.js build succeeds (`npm run build`)
2. MemoryDepthJudge follows same pattern as existing judges (BaseMetric, Zod schema, bedrockChatJSON)
3. A/B script produces two experiment runs with identical dataset but different prompt conditions
4. Results include memory_depth metric showing quality delta
5. Script handles edge cases (no full pass data, missing env vars)
</verification>

<success_criteria>
- MemoryDepthJudge in lib/evaluation/memory-quality-judge.ts scores deep memory usage
- CLI script at scripts/eval-memory-quality.ts runs A/B comparison: quick_ready vs full_pass
- Both conditions use 4 judges (personality_consistency, factuality, tone_matching, memory_depth)
- Comparison table printed to stdout with per-metric scores and deltas
- Script uses existing Opik infrastructure (evaluate, datasets, judges)
</success_criteria>

<output>
After completion, create `.planning/phases/04-cost-quality-measurement/04-02-SUMMARY.md`
</output>
