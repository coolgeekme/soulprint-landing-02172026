import { NextRequest, NextResponse } from "next/server";
import { createClient } from "@supabase/supabase-js";
import { createHash } from "crypto";
// import { gemini, DEFAULT_MODEL } from "@/lib/gemini/client"; // Removed
import { checkHealth, streamChatCompletion, chatCompletion, ChatMessage } from "@/lib/llm/local-client";
import { loadMemory, buildSystemPrompt } from "@/lib/letta/soulprint-memory";

// Initialize Supabase Admin client (to bypass RLS for key check)
const supabaseAdmin = createClient(
    process.env.NEXT_PUBLIC_SUPABASE_URL!,
    process.env.SUPABASE_SERVICE_ROLE_KEY! || process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!
);

export async function POST(req: NextRequest) {
    try {
        // 1. Extract API Key
        const authHeader = req.headers.get("authorization");
        if (!authHeader || !authHeader.startsWith("Bearer sk-soulprint-")) {
            return NextResponse.json({ error: "Missing or invalid API key" }, { status: 401 });
        }

        const rawKey = authHeader.replace("Bearer ", "");
        const hashedKey = createHash("sha256").update(rawKey).digest("hex");

        // 2. Validate API Key & Get User
        const { data: keyData, error: keyError } = await supabaseAdmin
            .from("api_keys")
            .select("user_id, id")
            .eq("key_hash", hashedKey)
            .single();

        if (keyError || !keyData) {
            return NextResponse.json({ error: "Invalid API key" }, { status: 401 });
        }

        // ===================================
        // üö¶ USAGE LIMIT CHECK (Gate Logic)
        // ===================================
        const { data: profile } = await supabaseAdmin
            .from('profiles')
            .select('usage_count, usage_limit')
            .eq('id', keyData.user_id)
            .single();

        const limit = profile?.usage_limit ?? 20; // Default hard limit
        const count = profile?.usage_count ?? 0;

        if (count >= limit) {
            return NextResponse.json({
                error: "SoulPrint Trial Limit Reached (20 Interactions). Access is currently restricted."
            }, { status: 403 });
        }

        // Increment Usage (Blocking to ensure enforcement)
        await supabaseAdmin
            .from('profiles')
            .update({ usage_count: count + 1 })
            .eq('id', keyData.user_id);


        // 3. Parse Request Body
        const body = await req.json();
        const { messages, model = 'hermes3', stream = false, soulprint_id } = body;

        // 4. Fetch User's SoulPrint (System Message)
        let targetSoulprintId = soulprint_id;

        // If no ID provided in body, check the user's "current" selection
        if (!targetSoulprintId) {
            const { data: profile } = await supabaseAdmin
                .from('profiles')
                .select('current_soulprint_id')
                .eq('id', keyData.user_id)
                .single();

            if (profile?.current_soulprint_id) {
                targetSoulprintId = profile.current_soulprint_id;
            }
        }

        let soulprintQuery = supabaseAdmin
            .from("soulprints")
            .select("soulprint_data")
            .eq("user_id", keyData.user_id);

        if (targetSoulprintId) {
            soulprintQuery = soulprintQuery.eq('id', targetSoulprintId);
        }

        const { data: soulprint } = await soulprintQuery.maybeSingle();

        // ============================================================
        // üöÄ UNIFIED LLM PATH (Local Hermes 3 -> Gemini Fallback)
        // ============================================================
        try {
            // Build Context
            let soulprintObj = soulprint?.soulprint_data;
            if (typeof soulprintObj === 'string') {
                try { soulprintObj = JSON.parse(soulprintObj); } catch (e) { }
            }

            // Prepare Messages (Prepend System Prompt if found)
            const memory = await loadMemory(keyData.user_id);
            const systemPrompt = buildSystemPrompt(soulprintObj, memory);

            const processedMessages: ChatMessage[] = [
                { role: 'system', content: systemPrompt },
                ...messages.map((m: any) => ({ role: m.role, content: m.content }))
            ];

            const { unifiedChatCompletion, unifiedStreamChatCompletion } = await import("@/lib/llm/unified-client");

            if (stream) {
                const streamResponse = new ReadableStream({
                    async start(controller) {
                        const encoder = new TextEncoder();
                        try {
                            for await (const chunk of unifiedStreamChatCompletion(processedMessages)) {
                                const data = JSON.stringify({
                                    choices: [{ delta: { content: chunk } }]
                                });
                                controller.enqueue(encoder.encode(`data: ${data}\n\n`));
                            }
                            controller.enqueue(encoder.encode("data: [DONE]\n\n"));
                        } catch (e) {
                            console.error("Streaming error:", e);
                        } finally {
                            controller.close();
                        }
                    }
                });

                return new Response(streamResponse, {
                    headers: {
                        "Content-Type": "text/event-stream",
                        "Cache-Control": "no-cache",
                        "Connection": "keep-alive",
                    },
                });
            }

            // Non-streaming path
            const content = await unifiedChatCompletion(processedMessages);

            return NextResponse.json({
                id: `chatcmpl-${Date.now()}`,
                object: 'chat.completion',
                created: Math.floor(Date.now() / 1000),
                model: 'soulprint-hybrid',
                choices: [{
                    index: 0,
                    message: { role: 'assistant', content },
                    finish_reason: 'stop'
                }]
            });

        } catch (llmError: any) {
            console.error('‚ùå LLM Generation Failed:', llmError);
            return NextResponse.json({
                error: `Generation failed: ${llmError.message || 'Unknown error'}`
            }, { status: 503 });
        }

    } catch (error: unknown) {
        console.error('‚ùå Chat API Error:', error);
        const errorMessage = error instanceof Error ? error.message : 'Unknown error';
        return NextResponse.json({
            error: `API error: ${errorMessage}`,
        }, { status: 500 });
    }
}
